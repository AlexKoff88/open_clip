{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_transform, eval_transform = open_clip.create_model_and_transforms('ViT-B-16-plus-240', pretrained='laion400m_e32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = open_clip.get_tokenizer('ViT-B-16-plus-240')\n",
    "\n",
    "image = eval_transform(Image.open(\"../docs/CLIP.png\")).unsqueeze(0)\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def get_pil_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "BACKUP_PAIR = (\n",
    "    get_pil_from_url(\n",
    "        \"https://thumbs.dreamstime.com/t/altai-mountains-mountain-lake-russia-siberia-chuya-ridge-49130812.jpg\"\n",
    "    ),\n",
    "    \"Altai mountains Stock Photography\",\n",
    ")\n",
    "AVAILABLE_EXAMPLES = []\n",
    "\n",
    "def check_text_data(data):\n",
    "    if isinstance(data, str):\n",
    "        return True\n",
    "    if isinstance(data, list):\n",
    "        return all(isinstance(x, str) for x in data)\n",
    "    return False    \n",
    "\n",
    "def laion2B_preprocess_train(examples, train_transforms, tokenize_captions, image_column=\"url\", text_column=\"caption\"):\n",
    "    url = examples[image_column]\n",
    "    try:\n",
    "        image = get_pil_from_url(url)\n",
    "        if not check_text_data(examples[text_column]):\n",
    "            raise ValueError(\"Text data is not valid\")\n",
    "        AVAILABLE_EXAMPLES.append((url, examples[text_column]))\n",
    "    except Exception:\n",
    "        print(f\"Can't load image from url: {url}, using cache with size: {len(AVAILABLE_EXAMPLES)}\")\n",
    "        if len(AVAILABLE_EXAMPLES) > 0:\n",
    "            backup_id = random.randint(0, len(AVAILABLE_EXAMPLES) - 1)\n",
    "            backup_example = AVAILABLE_EXAMPLES[backup_id]\n",
    "            try:\n",
    "                image = get_pil_from_url(backup_example[0])\n",
    "                examples[text_column] = backup_example[1]\n",
    "            except Exception:\n",
    "                print(f\"Can't load image from cached url: {backup_example[0]}, using backup\")\n",
    "                image = BACKUP_PAIR[0].copy()\n",
    "                examples[text_column] = BACKUP_PAIR[1]\n",
    "        else:\n",
    "            print(f\"Can't load image from url: {url}, using backup\")\n",
    "            image = BACKUP_PAIR[0].copy()\n",
    "            examples[text_column] = BACKUP_PAIR[1]\n",
    "\n",
    "    examples[\"pixel_values\"] = train_transforms(image)\n",
    "    examples[\"text\"] = tokenize_captions(examples)\n",
    "    return examples\n",
    "\n",
    "def tokenize_captions(examples, is_train=True):\n",
    "    caption_column = \"caption\"\n",
    "    captions = []\n",
    "    caption = examples[caption_column]\n",
    "    if isinstance(caption, str):\n",
    "        captions.append(caption)\n",
    "    elif isinstance(caption, (list, np.ndarray)):\n",
    "        # take a random caption if there are multiple\n",
    "        captions.append(random.choice(caption) if is_train else caption[0])\n",
    "    else:\n",
    "        raise ValueError(f\"Caption column `{caption_column}` should contain either strings or lists of strings.\")\n",
    "    #inputs = tokenizer(captions[0], max_length=tokenizer.model_max_length, padding=\"do_not_pad\", truncation=True)\n",
    "    #input_ids = inputs.input_ids\n",
    "    input_ids = tokenizer(captions[0])[0]\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "max_train_samples = 10000\n",
    "dataset = load_dataset(\"laion/laion400m\", streaming=True)\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42, buffer_size=max_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_dtype = model.transformer.get_cast_dtype()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    with torch.no_grad():\n",
    "        x = model.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n",
    "        x = x + model.positional_embedding.to(cast_dtype)\n",
    "    return x\n",
    "\n",
    "def collate_fn_image(examples):\n",
    "    examples = [laion2B_preprocess_train(example, train_transform, tokenize_captions) for example in examples]\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    \n",
    "    input_ids = torch.stack([preprocess_text(example[\"text\"]) for example in examples]).permute(1, 0, 2) # NLD -> LND\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_masks\": model.attn_mask,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def prepare_nncf_init_data(dataloader, init_steps):\n",
    "    nncf_init_data = []\n",
    "\n",
    "    print(f\"Fetching {init_steps} for the initialization...\")\n",
    "    for _, batch in tqdm(zip(range(init_steps), itertools.islice(dataloader, 0, init_steps))):\n",
    "        with torch.no_grad():\n",
    "            # Convert images to latent space\n",
    "            \n",
    "            nncf_init_data.append(\n",
    "                (\n",
    "                    batch[\"pixel_values\"].to(\"cpu\"),\n",
    "                    batch[\"input_ids\"].to(\"cpu\"),\n",
    "                    batch[\"attention_masks\"].to(\"cpu\")\n",
    "                )\n",
    "            )\n",
    "    return nncf_init_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 1\n",
    "dataloader_num_workers = 4\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, collate_fn=collate_fn_image, batch_size=train_batch_size, num_workers=dataloader_num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_init_steps = 10\n",
    "init_data = prepare_nncf_init_data(train_dataloader, opt_init_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.init_data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.init_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.init_data[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nncf\n",
    "\n",
    "def quantize_image_encoder(model, data_loader):\n",
    "    quantization_dataset = nncf.Dataset(data_loader, lambda x: x[0])\n",
    "\n",
    "    quantized_model = nncf.quantize(\n",
    "                            model,\n",
    "                            quantization_dataset,\n",
    "                            model_type=nncf.ModelType.TRANSFORMER,\n",
    "                            preset=nncf.QuantizationPreset.MIXED,\n",
    "                            \n",
    "                            )\n",
    "    return quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino.runtime as ov\n",
    "from pathlib import Path\n",
    "\n",
    "ov_model_path = Path(\"image_encoder.xml\")\n",
    "\n",
    "core = ov.Core()\n",
    "image_encoder = core.read_model(ov_model_path)\n",
    "\n",
    "q_image_encoder = quantize_image_encoder(image_encoder, InitDataset(init_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov.serialize(q_image_encoder, \"q_image_encoder.xml\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_text_encoder(model, data_loader):\n",
    "    quantization_dataset = nncf.Dataset(data_loader, lambda x: (x[1], x[2]))\n",
    "\n",
    "    quantized_model = nncf.quantize(\n",
    "                            model,\n",
    "                            quantization_dataset,\n",
    "                            model_type=nncf.ModelType.TRANSFORMER,\n",
    "                            preset=nncf.QuantizationPreset.MIXED,\n",
    "                            \n",
    "                            )\n",
    "    return quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model_path = Path(\"text_encoder.xml\")\n",
    "\n",
    "core = ov.Core()\n",
    "text_encoder = core.read_model(ov_model_path)\n",
    "\n",
    "q_text_encoder = quantize_text_encoder(text_encoder, InitDataset(init_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov.serialize(q_text_encoder, \"q_text_encoder.xml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomeov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
