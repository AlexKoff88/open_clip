{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nncf import NNCFConfig\n",
    "from nncf.common.logging import nncf_logger\n",
    "from nncf.torch import create_compressed_model, register_default_init_args\n",
    "from nncf.torch.initialization import PTInitializingDataLoader\n",
    "from nncf.torch.layer_utils import CompressionParameter\n",
    "\n",
    "from PIL import Image\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_transform, eval_transform = open_clip.create_model_and_transforms(\"ViT-B-16-plus-240\", pretrained=\"laion400m_e32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = open_clip.get_tokenizer('ViT-B-16-plus-240')\n",
    "\n",
    "image = eval_transform(Image.open(\"../../docs/CLIP.png\")).unsqueeze(0)\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def get_pil_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "BACKUP_PAIR = (\n",
    "    get_pil_from_url(\n",
    "        \"https://thumbs.dreamstime.com/t/altai-mountains-mountain-lake-russia-siberia-chuya-ridge-49130812.jpg\"\n",
    "    ),\n",
    "    \"Altai mountains Stock Photography\",\n",
    ")\n",
    "AVAILABLE_EXAMPLES = []\n",
    "\n",
    "def check_text_data(data):\n",
    "    if isinstance(data, str):\n",
    "        return True\n",
    "    if isinstance(data, list):\n",
    "        return all(isinstance(x, str) for x in data)\n",
    "    return False    \n",
    "\n",
    "def laion2B_preprocess_train(examples, train_transforms, tokenize_captions, image_column=\"url\", text_column=\"caption\"):\n",
    "    url = examples[image_column]\n",
    "    try:\n",
    "        image = get_pil_from_url(url)\n",
    "        if not check_text_data(examples[text_column]):\n",
    "            raise ValueError(\"Text data is not valid\")\n",
    "        AVAILABLE_EXAMPLES.append((url, examples[text_column]))\n",
    "    except Exception:\n",
    "        print(f\"Can't load image from url: {url}, using cache with size: {len(AVAILABLE_EXAMPLES)}\")\n",
    "        if len(AVAILABLE_EXAMPLES) > 0:\n",
    "            backup_id = random.randint(0, len(AVAILABLE_EXAMPLES) - 1)\n",
    "            backup_example = AVAILABLE_EXAMPLES[backup_id]\n",
    "            try:\n",
    "                image = get_pil_from_url(backup_example[0])\n",
    "                examples[text_column] = backup_example[1]\n",
    "            except Exception:\n",
    "                print(f\"Can't load image from cached url: {backup_example[0]}, using backup\")\n",
    "                image = BACKUP_PAIR[0].copy()\n",
    "                examples[text_column] = BACKUP_PAIR[1]\n",
    "        else:\n",
    "            print(f\"Can't load image from url: {url}, using backup\")\n",
    "            image = BACKUP_PAIR[0].copy()\n",
    "            examples[text_column] = BACKUP_PAIR[1]\n",
    "\n",
    "    examples[\"pixel_values\"] = train_transforms(image)\n",
    "    examples[\"input_ids\"] = tokenize_captions(examples)\n",
    "    return examples\n",
    "\n",
    "def tokenize_captions(examples, is_train=True):\n",
    "    caption_column = \"caption\"\n",
    "    captions = []\n",
    "    caption = examples[caption_column]\n",
    "    if isinstance(caption, str):\n",
    "        captions.append(caption)\n",
    "    elif isinstance(caption, (list, np.ndarray)):\n",
    "        # take a random caption if there are multiple\n",
    "        captions.append(random.choice(caption) if is_train else caption[0])\n",
    "    else:\n",
    "        raise ValueError(f\"Caption column `{caption_column}` should contain either strings or lists of strings.\")\n",
    "    #inputs = tokenizer(captions[0], max_length=tokenizer.model_max_length, padding=\"do_not_pad\", truncation=True)\n",
    "    #input_ids = inputs.input_ids\n",
    "    input_ids = tokenizer(captions[0])[0]\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "max_train_samples = 10000\n",
    "dataset = load_dataset(\"laion/laion400m\", streaming=True)\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42, buffer_size=max_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_image(examples):\n",
    "    examples = [laion2B_preprocess_train(example, train_transform, tokenize_captions) for example in examples]\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    \n",
    "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def prepare_nncf_init_data(dataloader, init_steps):\n",
    "    nncf_init_data = []\n",
    "\n",
    "    print(f\"Fetching {init_steps} for the initialization...\")\n",
    "    for _, batch in tqdm(zip(range(init_steps), itertools.islice(dataloader, 0, init_steps))):\n",
    "        with torch.no_grad():\n",
    "            # Convert images to latent space\n",
    "            \n",
    "            nncf_init_data.append(\n",
    "                (\n",
    "                    batch[\"pixel_values\"].to(\"cpu\"),\n",
    "                    batch[\"input_ids\"].to(\"cpu\")\n",
    "                )\n",
    "            )\n",
    "    return nncf_init_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 1\n",
    "dataloader_num_workers = 4\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, collate_fn=collate_fn_image, batch_size=train_batch_size, num_workers=dataloader_num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 300 for the initialization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb05affdac52434c8eaff6b808d3f09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't load image from url: https://s3.amazonaws.com/whataspace/space_pictures/pictures/000/033/180/fullwidth/_DSC0146.jpg?1591610094, using cache with size: 0\n",
      "Can't load image from url: https://s3.amazonaws.com/whataspace/space_pictures/pictures/000/033/180/fullwidth/_DSC0146.jpg?1591610094, using backup\n",
      "Can't load image from url: https://i0.wp.com/des.gearbest.com/uploads/pdm-desc-pic/Electronic/image/2016/11/25/1480056293210706.jpg?w=960, using cache with size: 2\n",
      "Can't load image from url: http://cdn3.static-homes.com/cgi-bin/readimage/9eb0ae4fa92b5dfd09b03c9e3dc997c5_1_resizeto_193x143x1, using cache with size: 3\n",
      "Can't load image from url: https://sslh.ulximg.com/image/740x493/cover/1533483761_ac8b9fbc89ca57a7269ec0c7f5947094.jpg/2143af354a3c53d14969369d2c6cbc04/1533483761_cc5f2e1e234c809fa4408488e0b19e4d.jpg, using cache with size: 7\n",
      "Can't load image from url: https://img.shellporn.com/spcs/thumbs/155/299_hotel_wang_.jpg, using cache with size: 5\n",
      "Can't load image from url: http://resizing.flixster.com/wp7S_BA23xLXC2iPn_Ozyuck-m8=/320x455/dkpu1ddg7pbsk.cloudfront.net/movie/26/93/269324_ori.jpg, using cache with size: 7\n",
      "Can't load image from url: http://dyn1.heritagestatic.com/lf?set=path%5B9%2F5%2F5%2F4%2F9554311%5D%2Csizedata%5B220x350%5D&call=url%5Bfile%3Aproduct.chain%5D, using cache with size: 8\n",
      "Can't load image from url: https://www.picclickimg.com/d/l400/pict/123866029585_/Ancient-Roman-Mosaic-Double-Face-Glass-Pendant-Bead.jpg, using cache with size: 8\n",
      "Can't load image from url: http://thehaengallery.com/wp-content/uploads/EasyRotatorStorage/user-content/erc_27_1368152757/content/assets/xfs_600x550_s80_xfs_600x550_s80_Grand_Central_Station%20Windows-0.jpg, using cache with size: 11\n",
      "Can't load image from url: https://cdn.shopify.com/s/files/1/1162/0686/products/10191845_J8083IvoryBack.jpg?v=1552659023, using cache with size: 13\n",
      "Can't load image from url: https://sep.yimg.com/ay/artbook/corinne-wasmuht-collagen-1986-2001-38.jpg, using cache with size: 13\n",
      "Can't load image from url: https://donthavetime-73c7.kxcdn.com/wp-content/uploads/2018/03/Depositphotos_180879716_original-1024x683.jpg, using cache with size: 13\n",
      "Can't load image from url: https://img.lxrco.com/1739MQ359/1560272-gucci-guccissima-joy-hobo-bag-guccissima-cream-leather-shoulder-bags-t6vinp0nqf.large.jpg, using cache with size: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't load image from url: http://i0.wp.com/www.ewindandsolar.com/i/2015/06/hy-380-led-puck-lights-for-cozy-living-room-design-led-puck-light-kit-under-cabinet-led-puck-lights-12v-led-puck-lights-led-puck-lights-120v-battery-powered-led-puck-lights-battery-led.jpg?w=200&strip=all, using cache with size: 18\n",
      "Can't load image from url: https://cdn1-www.momtastic.com/assets/uploads/2014/01/old-DIYs-e1389240447729.jpg, using cache with size: 22\n",
      "Can't load image from url: https://www.belleviemedical.com/wp-content/uploads/2016/03/Belle-Vie-Home-Face.png, using cache with size: 18\n",
      "Can't load image from url: http://s.en.fishki.net/upload/en/201211/23/2706/9840483e0be437697bcdb6501908d666.jpg, using cache with size: 19\n",
      "Can't load image from url: https://d2d00szk9na1qq.cloudfront.net/Product/0d387c2a-4687-4900-8861-17a0df831812/Images/Medium_0393423.jpg, using cache with size: 23\n",
      "Can't load image from url: https://d39eittn6ocpe0.cloudfront.net/media/catalog/product/cache/67df2192fc19fe888eca63b291423149/v/b/vb540b.jpg, using cache with size: 22\n",
      "Can't load image from url: http://www.china-mdf.com/attachment/2017/07/f64fff8792.jpg, using cache with size: 26\n",
      "Can't load image from url: https://pavoalhorno.com/wp-content/uploads/2019/05/patio-interesting-outdoor-lounge-chairs-clearance-pool-chair-cushions-cushion-1024x1024.jpg, using cache with size: 28\n",
      "Can't load image from url: https://dbcn1bdvswqbx.cloudfront.net/client_info/MIDWEST/itemimage/202070056/202070056-(1).jpg, using cache with size: 29\n",
      "Can't load image from url: http://ep.yimg.com/ca/I/trimfabric_2266_921227002, using cache with size: 30\n",
      "Can't load image from url: https://sep.yimg.com/ca/I/yhst-20066731852986_2271_2612542, using cache with size: 29\n",
      "Can't load image from url: http://sopooom.com/web/product/small/201811/a162b239544cf11f7d92381c58213139.jpg, using cache with size: 30\n",
      "Can't load image from url: https://www.mkwsurfaces.co.uk/wp-content/uploads/2019/07/Silestone-quartz-kitchen-worktops.jpg, using cache with size: 30\n",
      "Can't load image from url: http://product.nuji.com/medium/967d8d66-f9d0-4d51-8796-f7a3febe2579.jpg, using cache with size: 30\n",
      "Can't load image from url: https://bristolsport.azureedge.net/media/19729/dean-holden-pic.jpg?w=186&amp;h=122&amp;mode=crop&amp;scale=both, using cache with size: 34\n",
      "Can't load image from url: https://thumbs4.ebaystatic.com/d/l225/pict/152081476955_1.jpg, using cache with size: 35\n",
      "Can't load image from url: https://www.tcgakki.com/pic/sub_m/2/is186295341878820.jpg, using cache with size: 41\n",
      "Can't load image from url: http://thumbs4.ebaystatic.com/d/l300/pict/373422951157_1.jpg, using cache with size: 40\n",
      "Can't load image from url: https://ih0.redbubble.net/image.271525448.9871/mwo%2C187x204%2Cipad_2_snap-pad%2C210x230%2Cf8f8f8.lite-1.jpg, using cache with size: 41\n",
      "Can't load image from url: http://www.ehlingmedia.com/blog/wp-content/uploads/2014/03/Emilybronte_retouche-290x290.jpg, using cache with size: 47\n",
      "Can't load image from url: https://www.pacificrim.com.au/media/catalog/product/cache/1/small_image/170x/9df78eab33525d08d6e5fb8d27136e95/c/o/copy_of_s_zs10_1.jpg, using cache with size: 43\n",
      "Can't load image from url: http://nailartsdesign.com/wp-content/uploads/2014/12/Cycling-To-Lose-Weight-2-150x150.jpg, using cache with size: 48\n",
      "Can't load image from url: http://images-eu.amazon.com/images/P/1107481066.08.L.jpg, using cache with size: 48\n",
      "Can't load image from url: http://s3item.bookooinc.netdna-cdn.com/s160_47bb596bf168204537eaea8548995.jpg, using cache with size: 53\n",
      "Can't load image from url: https://media.istockphoto.com/vectors/vintage-1980s-style-item-set-vector-id544347962?k=6&amp;m=544347962&amp;s=612x612&amp;w=0&amp;h=I1TaRnnybQEaHuzAsW6qzPb2ZlASHwJweFql4YwMWmA=, using cache with size: 48\n",
      "Can't load image from url: https://static-eu.insales.ru/images/products/1/5439/194270527/1504861-6350-6-belden-pants-men-phantom.png, using cache with size: 48\n",
      "Can't load image from url: https://cdn.shopify.com/s/files/1/0081/7276/5246/products/2_05da28f6-05f6-46bf-81fe-9c2cf554107c_300x.jpg?v=1616707257, using cache with size: 54\n",
      "Can't load image from url: https://thx.sfo2.cdn.digitaloceanspaces.com/quotely_tqp/156/00156154.jpg, using cache with size: 53\n",
      "Can't load image from url: https://www.cityofboston.gov/Images_Documents/Ready_Boston_logo150_tcm3-10100.jpg, using cache with size: 50\n",
      "Can't load image from url: https://cdn2.mystore4.no/thumb/300_300/fitnessgrossisten/25957_Self_Omninutrition_Carnitine_Liquid_100000_-_1.jpg, using cache with size: 50\n",
      "Can't load image from url: https://cdn.shopify.com/s/files/1/0984/4632/products/long-rice-ball-mold-and-animal-cheese-cutter-set-with-case-home_300x.jpg?v=1527192559, using cache with size: 56\n",
      "Can't load image from url: https://cdn.shopify.com/s/files/1/0345/0689/products/16128-p1_1024x1024.jpg?v=1484859121, using cache with size: 59\n",
      "Can't load image from url: http://www.bickfordandsons.com.au/images/classic2/tonic-card.png, using cache with size: 52\n",
      "Can't load image from url: https://simages.wigsbuy.com/Upload/Image/2016/37/258-350/6795ed6f-5bfa-4ae6-9c80-11d68f33bc8e.jpg, using cache with size: 52\n",
      "Can't load image from url: https://pictures.dealer.com/s/subaruofgwinnettsoa/0111/671987bb186c859a8f55ea318d9d03bax.jpg?impolicy=resize&w=240, using cache with size: 52\n",
      "Can't load image from url: http://cdn1.englishbaby.com/dynamic/my_photo/thumbnail/0000/0001/1924/1924400_1289103443_123682.jpg, using cache with size: 59\n",
      "Can't load image from url: http://wtcno.org/wp-content/uploads/2018/11/WTC-NO-TAG_@600px.png, using cache with size: 59\n",
      "Can't load image from url: https://www.dealsanimg.com/d/l400/pict/224211762702_/apple-tv-3rd-generation-a1469-excellent-condition-rarely.jpg, using cache with size: 54\n",
      "Can't load image from url: http://ih0.redbubble.net/image.33508325.6752/ra,fitted_scoop,x1100,fafafa:ca443f4786,front-c,260,195,225,375-pad,220x294,ffffff.4u2.jpg, using cache with size: 55\n",
      "Can't load image from url: https://www.amerex.com.au/wp-content/uploads/Group-pic-at-end-of-presentation-png-312x222.jpg, using cache with size: 56\n"
     ]
    }
   ],
   "source": [
    "opt_init_steps = 300\n",
    "init_data = prepare_nncf_init_data(train_dataloader, opt_init_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't load image from url: http://ep.yimg.com/ay/yhst-83532116742892/cherry-blossom-birdies-pink-yellow-framed-art-print-8.jpg, using cache with size: 62\n"
     ]
    }
   ],
   "source": [
    "class InitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.init_data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.init_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.init_data[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply NNCF optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder_config = {\n",
    "    \"input_info\": [\n",
    "        {  \n",
    "            \"sample_size\": [1, 3, 240, 240]\n",
    "        },\n",
    "    ],\n",
    "    \"log_dir\": \"./\",  # The log directory for NNCF-specific logging outputs.\n",
    "    \"compression\": [\n",
    "        {\n",
    "            \"algorithm\": \"quantization\",  # Specify the algorithm here.\n",
    "            \"preset\": \"mixed\",\n",
    "            \"initializer\": {\n",
    "                \"range\": {\"num_init_samples\": opt_init_steps},\n",
    "                \"batchnorm_adaptation\": {\"num_bn_adaptation_samples\": opt_init_steps},\n",
    "            },\n",
    "            \"scope_overrides\": {\"activations\": {\"{re}.*__matmul___0\": {\"mode\": \"symmetric\"}, \"{re}.*mean_0\": {\"mode\": \"symmetric\"}}},\n",
    "            \"ignored_scopes\": [\n",
    "                \"{re}.*__add___.*\",\n",
    "                \"{re}.*layer_norm_0\",\n",
    "                \"{re}.*__truediv__*\",\n",
    "                \"{re}.*__mul___.*\",\n",
    "                \"{re}.*__matmul___1\",\n",
    "            ],\n",
    "            \"overflow_fix\": \"disable\",\n",
    "            \"export_to_onnx_standard_ops\": True,\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "init_dataloader = torch.utils.data.DataLoader(InitDataset(init_data), batch_size=1, num_workers=1)\n",
    "\n",
    "class ImageEncoderInitDataLoader(PTInitializingDataLoader):\n",
    "    def get_inputs(self, dataloader_output):\n",
    "        image = dataloader_output[0].float().to(\"cpu\", non_blocking=True)\n",
    "        return (image), {}\n",
    "\n",
    "    def get_target(self, dataloader_output):\n",
    "        return dataloader_output[0]\n",
    "\n",
    "image_encoder_config = NNCFConfig.from_dict(image_encoder_config)\n",
    "image_encoder_config = register_default_init_args(image_encoder_config, ImageEncoderInitDataLoader(init_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.\n",
      "  deprecate(\n"
     ]
    }
   ],
   "source": [
    "import tomeov\n",
    "\n",
    "tomeov.patch_openclip(model, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Not adding activation input quantizer for operation: 5 ToMeVisionTransformer/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 6 ToMeVisionTransformer/LayerNorm[ln_pre]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 7 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[0]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 16 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[0]/ToMeAttention[attn]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 19 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[0]/ToMeAttention[attn]/__matmul___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 25 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[0]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 40 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[0]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 27 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[0]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 52 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[0]/__truediv___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 53 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[0]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 57 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[0]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 58 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[1]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 67 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[1]/ToMeAttention[attn]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 70 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[1]/ToMeAttention[attn]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 73 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[1]/ToMeAttention[attn]/__matmul___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 79 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[1]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 93 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[1]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 81 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[1]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 110 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[1]/__truediv___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 111 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[1]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 115 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[1]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 116 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[2]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 125 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[2]/ToMeAttention[attn]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 128 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[2]/ToMeAttention[attn]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 131 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[2]/ToMeAttention[attn]/__matmul___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 137 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[2]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 139 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[2]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 151 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[2]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 168 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[2]/__truediv___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 169 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[2]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 173 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[2]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 174 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[3]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 183 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[3]/ToMeAttention[attn]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 186 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[3]/ToMeAttention[attn]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 189 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[3]/ToMeAttention[attn]/__matmul___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 195 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[3]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 197 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[3]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 209 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[3]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 226 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[3]/__truediv___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 227 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[3]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 231 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[3]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 232 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[4]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 241 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[4]/ToMeAttention[attn]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 244 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[4]/ToMeAttention[attn]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 247 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[4]/ToMeAttention[attn]/__matmul___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 253 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[4]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 255 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[4]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 267 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[4]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 284 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[4]/__truediv___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 285 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[4]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 289 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[4]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 290 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[5]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 299 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[5]/ToMeAttention[attn]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 302 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[5]/ToMeAttention[attn]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 305 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[5]/ToMeAttention[attn]/__matmul___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 311 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[5]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 313 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[5]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 325 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[5]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 342 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[5]/__truediv___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 343 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[5]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 347 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[5]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 348 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[6]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 357 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[6]/ToMeAttention[attn]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 360 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[6]/ToMeAttention[attn]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 363 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[6]/ToMeAttention[attn]/__matmul___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 369 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[6]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 371 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[6]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 383 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[6]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 400 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[6]/__truediv___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 401 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[6]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 405 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[6]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 406 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[7]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 415 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[7]/ToMeAttention[attn]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 418 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[7]/ToMeAttention[attn]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 421 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[7]/ToMeAttention[attn]/__matmul___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 427 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[7]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 429 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[7]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 441 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[7]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 458 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[7]/__truediv___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 459 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[7]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 463 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[7]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 464 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[8]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 473 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[8]/ToMeAttention[attn]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 476 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[8]/ToMeAttention[attn]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 479 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[8]/ToMeAttention[attn]/__matmul___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 485 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[8]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 487 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[8]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 499 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[8]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 516 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[8]/__truediv___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 517 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[8]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 521 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[8]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 522 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[9]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 531 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[9]/ToMeAttention[attn]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 534 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[9]/ToMeAttention[attn]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 537 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[9]/ToMeAttention[attn]/__matmul___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 543 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[9]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 545 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[9]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 557 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[9]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 574 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[9]/__truediv___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 575 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[9]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 579 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[9]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 580 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[10]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 589 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[10]/ToMeAttention[attn]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 592 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[10]/ToMeAttention[attn]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 595 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[10]/ToMeAttention[attn]/__matmul___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 601 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[10]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 603 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[10]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 615 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[10]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 632 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[10]/__truediv___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 633 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[10]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 637 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[10]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 638 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[11]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 647 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[11]/ToMeAttention[attn]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 650 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[11]/ToMeAttention[attn]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 653 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[11]/ToMeAttention[attn]/__matmul___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 659 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[11]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 661 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[11]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 673 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[11]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 690 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[11]/__truediv___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 691 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[11]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 695 ToMeVisionTransformer/Transformer[transformer]/ModuleList[resblocks]/ToMeResidualAttentionBlock[11]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 698 ToMeVisionTransformer/LayerNorm[ln_post]/layer_norm_0\n",
      "INFO:nncf:Collecting tensor statistics |█               | 33 / 300\n",
      "INFO:nncf:Collecting tensor statistics |███             | 66 / 300\n",
      "INFO:nncf:Collecting tensor statistics |█████           | 99 / 300\n",
      "INFO:nncf:Collecting tensor statistics |███████         | 132 / 300\n",
      "INFO:nncf:Collecting tensor statistics |████████        | 165 / 300\n",
      "INFO:nncf:Collecting tensor statistics |██████████      | 198 / 300\n",
      "INFO:nncf:Collecting tensor statistics |████████████    | 231 / 300\n",
      "INFO:nncf:Collecting tensor statistics |██████████████  | 264 / 300\n",
      "INFO:nncf:Collecting tensor statistics |███████████████ | 297 / 300\n",
      "INFO:nncf:Collecting tensor statistics |████████████████| 300 / 300\n",
      "INFO:nncf:Compiling and loading torch extension: quantized_functions_cpu...\n",
      "INFO:nncf:Finished loading torch extension: quantized_functions_cpu\n",
      "INFO:nncf:BatchNorm statistics adaptation |█               | 33 / 300\n",
      "INFO:nncf:BatchNorm statistics adaptation |███             | 66 / 300\n",
      "INFO:nncf:BatchNorm statistics adaptation |█████           | 99 / 300\n",
      "INFO:nncf:BatchNorm statistics adaptation |███████         | 132 / 300\n",
      "INFO:nncf:BatchNorm statistics adaptation |████████        | 165 / 300\n",
      "INFO:nncf:BatchNorm statistics adaptation |██████████      | 198 / 300\n",
      "INFO:nncf:BatchNorm statistics adaptation |████████████    | 231 / 300\n",
      "INFO:nncf:BatchNorm statistics adaptation |██████████████  | 264 / 300\n",
      "INFO:nncf:BatchNorm statistics adaptation |███████████████ | 297 / 300\n",
      "INFO:nncf:BatchNorm statistics adaptation |████████████████| 300 / 300\n"
     ]
    }
   ],
   "source": [
    "image_controller, image_encoder = create_compressed_model(model.visual, image_encoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nncf.torch.dynamic_graph.io_handling import wrap_nncf_model_inputs_with_objwalk\n",
    "from nncf.config.structures import BNAdaptationInitArgs\n",
    "from nncf.config.structures import QuantizationRangeInitArgs\n",
    "\n",
    "text_encoder_config_dict = {\n",
    "    \"input_info\": [\n",
    "        {  \n",
    "            \"sample_size\": [640, 1, 77],\n",
    "        },\n",
    "        {\n",
    "            \"sample_size\": [77, 77],\n",
    "        }\n",
    "    ],\n",
    "    \"log_dir\": \"./\",  # The log directory for NNCF-specific logging outputs.\n",
    "    \"compression\": [\n",
    "        {\n",
    "            \"algorithm\": \"quantization\",  # Specify the algorithm here.\n",
    "            \"preset\": \"mixed\",\n",
    "            \"initializer\": {\n",
    "                \"range\": {\"num_init_samples\": opt_init_steps},\n",
    "                \"batchnorm_adaptation\": {\"num_bn_adaptation_samples\": opt_init_steps},\n",
    "            },\n",
    "            \"scope_overrides\": {\"activations\": {\"{re}.*baddbmm_0\": {\"mode\": \"symmetric\"}}},\n",
    "            \"ignored_scopes\": [\n",
    "                \"{re}.*__add___.*\",\n",
    "                \"{re}.*layer_norm_.*\",\n",
    "                \"{re}.*__truediv__*\",\n",
    "                \"{re}.*/bmm_0\",\n",
    "            ],\n",
    "            \"overflow_fix\": \"disable\",\n",
    "            \"export_to_onnx_standard_ops\": True,\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "class TextEncoderInitDataLoader(PTInitializingDataLoader):\n",
    "    \"\"\"\n",
    "    This class wraps the nncf.Dataset.\n",
    "\n",
    "    This is required for proper initialization of certain compression algorithms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_loader):\n",
    "        super().__init__(data_loader)\n",
    "        self._length = None\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._data_loader)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self._length is None:\n",
    "            data = self._data_loader\n",
    "            self._length = TextEncoderInitDataLoader._get_length(data)\n",
    "        return self._length\n",
    "\n",
    "    def get_inputs(self, dataloader_output):\n",
    "        with torch.no_grad():\n",
    "            text_embeddings = dataloader_output[1].to(\"cpu\", non_blocking=True)\n",
    "            #text_embeddings = torch.squeeze(text_embeddings, 0)\n",
    "            print(f\"text_embeddings.shape: {text_embeddings.shape}\")\n",
    "            x = model.token_embedding(text_embeddings)\n",
    "            x = x + model.positional_embedding\n",
    "            print(f\"x.hape: {x.shape}\")\n",
    "            x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "            print(f\"x.hape: {x.shape}\")\n",
    "        return (x, model.attn_mask), {}#{\"x\": x, \"attn_mask\": model.attn_mask}\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_length(iterable) -> int:\n",
    "        length = 0\n",
    "        for _ in iterable:\n",
    "            length = length + 1\n",
    "\n",
    "        return length\n",
    "\n",
    "# class TextEncoderInitDataLoader(PTInitializingDataLoader):\n",
    "#     def get_inputs(self, dataloader_output):\n",
    "#         with torch.no_grad():\n",
    "#             text_embeddings = dataloader_output[1].to(\"cpu\", non_blocking=True)\n",
    "#             text_embeddings = torch.squeeze(text_embeddings, 0)\n",
    "#             print(f\"text_embeddings.shape: {text_embeddings.shape}\")\n",
    "#             x = model.token_embedding(text_embeddings)\n",
    "#             x = x + model.positional_embedding\n",
    "#             print(f\"x.hape: {x.shape}\")\n",
    "#             x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "#             print(f\"x.hape: {x.shape}\")\n",
    "#         return (x, model.attn_mask), {}#{\"x\": x, \"attn_mask\": model.attn_mask}\n",
    "\n",
    "text_encoder_config = NNCFConfig.from_dict(text_encoder_config_dict)\n",
    "text_encoder_dataloader = TextEncoderInitDataLoader(init_data)\n",
    "# text_encoder_config = register_default_init_args(text_encoder_config, text_encoder_dataloader)\n",
    "text_encoder_config.register_extra_structs(\n",
    "        [\n",
    "            QuantizationRangeInitArgs(data_loader=text_encoder_dataloader),\n",
    "            BNAdaptationInitArgs(data_loader=text_encoder_dataloader),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, resblock in enumerate(model.transformer.resblocks):\n",
    "#     attn = tomeov.openclip.ToMeAttention(resblock.attn.embed_dim, resblock.attn.num_heads, qkv_bias=True)\n",
    "#     _, device = tomeov.openclip.convert_attention_block(resblock.attn, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_embeddings.shape: torch.Size([1, 77])\n",
      "x.hape: torch.Size([1, 77, 640])\n",
      "x.hape: torch.Size([77, 1, 640])\n",
      "text_embeddings.shape: torch.Size([1, 77])\n",
      "x.hape: torch.Size([1, 77, 640])\n",
      "x.hape: torch.Size([77, 1, 640])\n",
      "text_embeddings.shape: torch.Size([1, 77])\n",
      "x.hape: torch.Size([1, 77, 640])\n",
      "x.hape: torch.Size([77, 1, 640])\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 2 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[0]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 16 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[0]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 26 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[0]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 27 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[0]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 31 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[0]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 32 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[1]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 46 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[1]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 56 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[1]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 57 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[1]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 61 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[1]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 62 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[2]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 76 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[2]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 86 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[2]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 87 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[2]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 91 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[2]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 92 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[3]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 106 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[3]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 116 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[3]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 117 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[3]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 121 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[3]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 122 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[4]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 136 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[4]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 146 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[4]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 147 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[4]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 151 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[4]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 152 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[5]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 166 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[5]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 176 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[5]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 177 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[5]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 181 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[5]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 182 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[6]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 196 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[6]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 206 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[6]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 207 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[6]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 211 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[6]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 212 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[7]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 226 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[7]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 236 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[7]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 237 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[7]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 241 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[7]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 242 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[8]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 256 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[8]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 266 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[8]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 267 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[8]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 271 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[8]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 272 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[9]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 286 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[9]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 296 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[9]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 297 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[9]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 301 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[9]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 302 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[10]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 316 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[10]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 326 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[10]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 327 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[10]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 331 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[10]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 332 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[11]/LayerNorm[ln_1]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 346 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[11]/MultiheadAttention[attn]/__truediv___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 356 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[11]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 357 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[11]/LayerNorm[ln_2]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 361 Transformer/ModuleList[resblocks]/ResidualAttentionBlock[11]/__add___1\n",
      "text_embeddings.shape: torch.Size([1, 77])\n",
      "x.hape: torch.Size([1, 77, 640])\n",
      "x.hape: torch.Size([77, 1, 640])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m dummy_forward\n\u001b[1;32m     34\u001b[0m dummy_forward_fn \u001b[39m=\u001b[39m create_dummy_forward_fn(text_encoder_dataloader, get_model_device(model))\n\u001b[0;32m---> 36\u001b[0m text_controller, text_encoder \u001b[39m=\u001b[39m create_compressed_model(\n\u001b[1;32m     37\u001b[0m     model\u001b[39m.\u001b[39;49mtransformer, \n\u001b[1;32m     38\u001b[0m     text_encoder_config, \n\u001b[1;32m     39\u001b[0m     dummy_forward_fn\u001b[39m=\u001b[39;49mdummy_forward_fn,\n\u001b[1;32m     40\u001b[0m     wrap_inputs_fn\u001b[39m=\u001b[39;49mwrap_inputs,\n\u001b[1;32m     41\u001b[0m     wrap_outputs_fn\u001b[39m=\u001b[39;49mwrap_outputs,)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/telemetry/decorator.py:71\u001b[0m, in \u001b[0;36mtracked_function.__call__.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m event \u001b[39min\u001b[39;00m events:\n\u001b[1;32m     64\u001b[0m         telemetry\u001b[39m.\u001b[39msend_event(\n\u001b[1;32m     65\u001b[0m             event_category\u001b[39m=\u001b[39mcategory,\n\u001b[1;32m     66\u001b[0m             event_action\u001b[39m=\u001b[39mevent\u001b[39m.\u001b[39mname,\n\u001b[1;32m     67\u001b[0m             event_label\u001b[39m=\u001b[39mevent\u001b[39m.\u001b[39mdata,\n\u001b[1;32m     68\u001b[0m             event_value\u001b[39m=\u001b[39mevent\u001b[39m.\u001b[39mint_data,\n\u001b[1;32m     69\u001b[0m         )\n\u001b[0;32m---> 71\u001b[0m retval \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m category \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m category \u001b[39m!=\u001b[39m previous_category:\n\u001b[1;32m     74\u001b[0m     telemetry\u001b[39m.\u001b[39mend_session(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_category)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/model_creation.py:129\u001b[0m, in \u001b[0;36mcreate_compressed_model\u001b[0;34m(model, config, compression_state, dummy_forward_fn, wrap_inputs_fn, wrap_outputs_fn, dump_graphs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m is_state_loadable:\n\u001b[1;32m    128\u001b[0m     builder\u001b[39m.\u001b[39mload_state(compression_state[BaseController\u001b[39m.\u001b[39mBUILDER_STATE])\n\u001b[0;32m--> 129\u001b[0m compressed_model \u001b[39m=\u001b[39m builder\u001b[39m.\u001b[39;49mapply_to(nncf_network)\n\u001b[1;32m    130\u001b[0m compression_ctrl \u001b[39m=\u001b[39m builder\u001b[39m.\u001b[39mbuild_controller(compressed_model)\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m is_state_loadable:\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/compression_method_api.py:123\u001b[0m, in \u001b[0;36mPTCompressionAlgorithmBuilder.apply_to\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_to\u001b[39m(\u001b[39mself\u001b[39m, model: NNCFNetwork) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NNCFNetwork:\n\u001b[1;32m    122\u001b[0m     transformer \u001b[39m=\u001b[39m PTModelTransformer(model)\n\u001b[0;32m--> 123\u001b[0m     transformation_layout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_transformation_layout(model)\n\u001b[1;32m    124\u001b[0m     transformed_model \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mtransform(transformation_layout)\n\u001b[1;32m    126\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_init:\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/compression_method_api.py:142\u001b[0m, in \u001b[0;36mPTCompressionAlgorithmBuilder.get_transformation_layout\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39mApplies algorithm-specific modifications to the model. Hooks to be executed during model\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39mforward operation may be registered using NNCFNetwork command insertion methods. Additional\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m:return: NNCFNetwork with algorithm-specific modifications applied\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m check_scopes_in_graph(model\u001b[39m.\u001b[39mnncf\u001b[39m.\u001b[39mget_original_graph(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignored_scopes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_scopes)\n\u001b[0;32m--> 142\u001b[0m layout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_transformation_layout(model)\n\u001b[1;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_frozen_layers(model)\n\u001b[1;32m    144\u001b[0m \u001b[39mreturn\u001b[39;00m layout\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/quantization/algo.py:622\u001b[0m, in \u001b[0;36mQuantizationBuilder._get_transformation_layout\u001b[0;34m(self, target_model)\u001b[0m\n\u001b[1;32m    620\u001b[0m target_model\u001b[39m.\u001b[39mnncf\u001b[39m.\u001b[39mregister_compression_module_type(ExtraCompressionModuleType\u001b[39m.\u001b[39mEXTERNAL_QUANTIZER)\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pt_quantizer_setup \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pt_quantizer_setup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_quantizer_setup(target_model)\n\u001b[1;32m    624\u001b[0m dup_filter \u001b[39m=\u001b[39m DuplicateFilter()  \u001b[39m# so that the overflow fix warning is only logged once\u001b[39;00m\n\u001b[1;32m    625\u001b[0m nncf_logger\u001b[39m.\u001b[39maddFilter(dup_filter)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/quantization/algo.py:715\u001b[0m, in \u001b[0;36mQuantizationBuilder._get_quantizer_setup\u001b[0;34m(self, target_model)\u001b[0m\n\u001b[1;32m    712\u001b[0m target_model_graph \u001b[39m=\u001b[39m target_model\u001b[39m.\u001b[39mnncf\u001b[39m.\u001b[39mget_original_graph()\n\u001b[1;32m    714\u001b[0m \u001b[39mif\u001b[39;00m is_main_process() \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_init:\n\u001b[0;32m--> 715\u001b[0m     stats_for_range_init \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_statistics_for_final_range_init(\n\u001b[1;32m    716\u001b[0m         target_model, single_config_quantizer_setup, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_range_init_params\n\u001b[1;32m    717\u001b[0m     )\n\u001b[1;32m    718\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_minmax_values_for_range_init \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_minmax_values_for_quantizer_locations(\n\u001b[1;32m    719\u001b[0m         single_config_quantizer_setup, stats_for_range_init, target_model_graph\n\u001b[1;32m    720\u001b[0m     )\n\u001b[1;32m    722\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_and_log_missing_stats_for_setup(\n\u001b[1;32m    723\u001b[0m         single_config_quantizer_setup, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_minmax_values_for_range_init\n\u001b[1;32m    724\u001b[0m     )\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/quantization/algo.py:688\u001b[0m, in \u001b[0;36mQuantizationBuilder._get_statistics_for_final_range_init\u001b[0;34m(self, target_model, quantizer_setup, range_init_params)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_statistics_for_final_range_init\u001b[39m(\n\u001b[1;32m    686\u001b[0m     \u001b[39mself\u001b[39m, target_model: NNCFNetwork, quantizer_setup: QuantizerSetupBase, range_init_params: PTRangeInitParams\n\u001b[1;32m    687\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[PTTargetPoint, Dict[ReductionShape, TensorStatistic]]:\n\u001b[0;32m--> 688\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_statistics_for_quantizer_setup(target_model, quantizer_setup, range_init_params)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/quantization/algo.py:672\u001b[0m, in \u001b[0;36mQuantizationBuilder.get_statistics_for_quantizer_setup\u001b[0;34m(target_model, quantizer_setup, range_init_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[39mreturn\u001b[39;00m {}\n\u001b[1;32m    666\u001b[0m observation_points_vs_collectors_dict \u001b[39m=\u001b[39m (\n\u001b[1;32m    667\u001b[0m     StatCollectorGenerator\u001b[39m.\u001b[39mgenerate_collectors_for_range_init_statistics_collection(\n\u001b[1;32m    668\u001b[0m         target_model\u001b[39m.\u001b[39mnncf\u001b[39m.\u001b[39mget_original_graph(), quantizer_setup, range_init_params\n\u001b[1;32m    669\u001b[0m     )\n\u001b[1;32m    670\u001b[0m )\n\u001b[0;32m--> 672\u001b[0m \u001b[39mwith\u001b[39;00m target_model\u001b[39m.\u001b[39mnncf\u001b[39m.\u001b[39mtemporary_clean_view() \u001b[39mas\u001b[39;00m intermediate_model:\n\u001b[1;32m    673\u001b[0m     stat_builder \u001b[39m=\u001b[39m TensorStatisticsCollectionBuilder(NNCFConfig(), observation_points_vs_collectors_dict)\n\u001b[1;32m    674\u001b[0m     stat_builder\u001b[39m.\u001b[39mapply_to(intermediate_model)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/nncf_network.py:684\u001b[0m, in \u001b[0;36mNNCFNetworkInterface.temporary_clean_view.<locals>.Mgr.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnncf_module_state_dicts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnncf\u001b[39m.\u001b[39msave_nncf_module_additions()\n\u001b[1;32m    683\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnncf_interface \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnncf\n\u001b[0;32m--> 684\u001b[0m clean_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mnncf\u001b[39m.\u001b[39;49mget_clean_shallow_copy()\n\u001b[1;32m    685\u001b[0m \u001b[39mreturn\u001b[39;00m clean_model\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/nncf_network.py:351\u001b[0m, in \u001b[0;36mNNCFNetworkInterface.get_clean_shallow_copy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnncf\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m save_module_state\n\u001b[1;32m    350\u001b[0m saved_state \u001b[39m=\u001b[39m save_module_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_ref)\n\u001b[0;32m--> 351\u001b[0m new_interface \u001b[39m=\u001b[39m NNCFNetworkInterface(\n\u001b[1;32m    352\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model_ref,\n\u001b[1;32m    353\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_infos,\n\u001b[1;32m    354\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_user_dummy_forward_fn,\n\u001b[1;32m    355\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_inputs_fn,\n\u001b[1;32m    356\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scopes_without_shape_matching,\n\u001b[1;32m    357\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ignored_scopes,\n\u001b[1;32m    358\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_target_scopes,\n\u001b[1;32m    359\u001b[0m     wrap_outputs_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_outputs_fn,\n\u001b[1;32m    360\u001b[0m )\n\u001b[1;32m    361\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_ref\u001b[39m.\u001b[39m_nncf \u001b[39m=\u001b[39m new_interface  \u001b[39m# pylint:disable=protected-access\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_ref\u001b[39m.\u001b[39mnncf\u001b[39m.\u001b[39mreset_nncf_modules()\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/nncf_network.py:255\u001b[0m, in \u001b[0;36mNNCFNetworkInterface.__init__\u001b[0;34m(self, model, input_infos, dummy_forward_fn, wrap_inputs_fn, scopes_without_shape_matching, ignored_scopes, target_scopes, wrap_outputs_fn)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insertions_into_original_graph \u001b[39m=\u001b[39m (\n\u001b[1;32m    248\u001b[0m     {}\n\u001b[1;32m    249\u001b[0m )  \u001b[39m# type: Dict[PTTargetPoint, List[Tuple[Callable, TransformationPriority]]]\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _orig_graph_build_forward_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_dummy_forward_fn_for_graph_building(\n\u001b[1;32m    252\u001b[0m     with_input_tracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, with_output_tracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    253\u001b[0m )\n\u001b[0;32m--> 255\u001b[0m eval_op_scopes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_collect_eval_op_scopes(model, _orig_graph_build_forward_fn)\n\u001b[1;32m    257\u001b[0m \u001b[39m# all modules called in eval mode should be replaced prior to graph building\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replace_modules_by_nncf_modules(model, device, eval_op_scopes)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/nncf_network.py:700\u001b[0m, in \u001b[0;36mNNCFNetworkInterface._collect_eval_op_scopes\u001b[0;34m(self, model, dummy_forward_fn)\u001b[0m\n\u001b[1;32m    698\u001b[0m tracer \u001b[39m=\u001b[39m GraphTracer(dummy_forward_fn)\n\u001b[1;32m    699\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 700\u001b[0m eval_graph \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39;49mtrace_graph(model, as_eval\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    701\u001b[0m root_scope \u001b[39m=\u001b[39m Scope()\n\u001b[1;32m    702\u001b[0m \u001b[39mfor\u001b[39;00m dyn_graph_node \u001b[39min\u001b[39;00m eval_graph\u001b[39m.\u001b[39mget_all_nodes():\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/graph_tracer.py:113\u001b[0m, in \u001b[0;36mGraphTracer.trace_graph\u001b[0;34m(self, model, context_to_use, as_eval)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m as_eval:\n\u001b[1;32m    112\u001b[0m     \u001b[39mwith\u001b[39;00m training_mode_switcher(model, is_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 113\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcustom_forward_fn(model)\n\u001b[1;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcustom_forward_fn(model)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/nncf_network.py:438\u001b[0m, in \u001b[0;36mNNCFNetworkInterface._get_dummy_forward_fn_for_graph_building.<locals>.wrapped_user_dummy_forward_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_user_dummy_forward_fn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    437\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_user_dummy_forward \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m     retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_user_dummy_forward_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_user_dummy_forward \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m retval\n",
      "Cell \u001b[0;32mIn[16], line 27\u001b[0m, in \u001b[0;36mcreate_dummy_forward_fn.<locals>.dummy_forward\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     24\u001b[0m     args \u001b[39m=\u001b[39m objwalk(args, is_tensor, send_to_device)\n\u001b[1;32m     25\u001b[0m     kwargs \u001b[39m=\u001b[39m objwalk(kwargs, is_tensor, send_to_device)\n\u001b[0;32m---> 27\u001b[0m args, kwargs \u001b[39m=\u001b[39m wrap_inputs(args, kwargs)\n\u001b[1;32m     28\u001b[0m retval \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     29\u001b[0m retval \u001b[39m=\u001b[39m replicate_same_tensors(retval)\n",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m, in \u001b[0;36mwrap_inputs\u001b[0;34m(args, kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_inputs\u001b[39m(args, kwargs):\n\u001b[0;32m---> 10\u001b[0m         \u001b[39mreturn\u001b[39;00m wrap_nncf_model_inputs_with_objwalk(args, kwargs)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/io_handling.py:35\u001b[0m, in \u001b[0;36mwrap_nncf_model_inputs_with_objwalk\u001b[0;34m(model_args, model_kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_nncf_model_inputs_with_objwalk\u001b[39m(model_args, model_kwargs):\n\u001b[0;32m---> 35\u001b[0m     model_args \u001b[39m=\u001b[39m objwalk(model_args, is_tensor, nncf_model_input)\n\u001b[1;32m     36\u001b[0m     model_kwargs \u001b[39m=\u001b[39m objwalk(model_kwargs, is_tensor, nncf_model_input)\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m model_args, model_kwargs\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/nested_objects_traversal.py:161\u001b[0m, in \u001b[0;36mobjwalk\u001b[0;34m(obj, unary_predicate, apply_fn, memo)\u001b[0m\n\u001b[1;32m    159\u001b[0m             objwalk(value, unary_predicate, apply_fn)\n\u001b[1;32m    160\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices_to_apply_fn_to:\n\u001b[0;32m--> 161\u001b[0m     obj[idx] \u001b[39m=\u001b[39m apply_fn(obj[idx])\n\u001b[1;32m    162\u001b[0m \u001b[39mfor\u001b[39;00m idx, tpl_data \u001b[39min\u001b[39;00m indices_vs_named_tuple_data\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    163\u001b[0m     tpl, n_tpl_class, n_tpl_fields \u001b[39m=\u001b[39m tpl_data\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:94\u001b[0m, in \u001b[0;36mwrap_operator.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     result \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39mtensor_cache\n\u001b[1;32m     93\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     result \u001b[39m=\u001b[39m _execute_op(op_address, operator_info, operator, ctx, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     96\u001b[0m str_op_address \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(op_address)\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m str_op_address \u001b[39min\u001b[39;00m ctx\u001b[39m.\u001b[39mend_node_name_of_skipped_block:\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:181\u001b[0m, in \u001b[0;36m_execute_op\u001b[0;34m(op_address, operator_info, operator, ctx, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39melif\u001b[39;00m ctx\u001b[39m.\u001b[39mtrace_dynamic_graph:\n\u001b[1;32m    180\u001b[0m     tensor_metas \u001b[39m=\u001b[39m make_tensor_metas(processed_input)\n\u001b[0;32m--> 181\u001b[0m     node \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39;49mfind_operator_node(tensor_metas, op_address)\n\u001b[1;32m    182\u001b[0m     \u001b[39mif\u001b[39;00m node \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m         layer_attrs, ignored_algos \u001b[39m=\u001b[39m _collect_module_attrs_and_ignored_algorithms(ctx, op_name, args, kwargs)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/context.py:150\u001b[0m, in \u001b[0;36mTracingContext.find_operator_node\u001b[0;34m(self, tensor_metas, op_address)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threading\u001b[39m.\u001b[39mcond:\n\u001b[1;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_instances_searching_graph \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 150\u001b[0m node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgraph\u001b[39m.\u001b[39;49mfind_node(op_address, tensor_metas, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_comparators_per_scope)\n\u001b[1;32m    152\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threading\u001b[39m.\u001b[39mcond:\n\u001b[1;32m    153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_instances_searching_graph \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/graph.py:610\u001b[0m, in \u001b[0;36mDynamicGraph.find_node\u001b[0;34m(self, op_address, tensor_metas, input_comparators_per_scope)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_node\u001b[39m(\n\u001b[1;32m    605\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    606\u001b[0m     op_address: OperationAddress,\n\u001b[1;32m    607\u001b[0m     tensor_metas: List[TensorMeta],\n\u001b[1;32m    608\u001b[0m     input_comparators_per_scope: List[Tuple[TensorMetaComparator, List[\u001b[39mstr\u001b[39m]]],\n\u001b[1;32m    609\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DynamicGraphNode:\n\u001b[0;32m--> 610\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmatch_manager\u001b[39m.\u001b[39;49mfind_node(op_address, tensor_metas, input_comparators_per_scope)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/graph.py:536\u001b[0m, in \u001b[0;36mNodeManager.find_node\u001b[0;34m(self, op_address, tensor_metas, input_comparators_per_scope)\u001b[0m\n\u001b[1;32m    534\u001b[0m matcher \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchoose_matcher(op_address)\n\u001b[1;32m    535\u001b[0m comparators \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchoose_tm_comparators(op_address, input_comparators_per_scope)\n\u001b[0;32m--> 536\u001b[0m \u001b[39mreturn\u001b[39;00m matcher\u001b[39m.\u001b[39;49mfind_node(op_address, tensor_metas, comparators)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/graph.py:354\u001b[0m, in \u001b[0;36mDefaultScopeNodeMatcher.find_node\u001b[0;34m(self, op_address, tensor_metas, tm_comparators)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_node\u001b[39m(\n\u001b[1;32m    345\u001b[0m     \u001b[39mself\u001b[39m, op_address: OperationAddress, tensor_metas: List[TensorMeta], tm_comparators: List[TensorMetaComparator]\n\u001b[1;32m    346\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DynamicGraphNode:\n\u001b[1;32m    347\u001b[0m     op_exec_context \u001b[39m=\u001b[39m OperationExecutionContext(\n\u001b[1;32m    348\u001b[0m         op_address\u001b[39m.\u001b[39moperator_name,\n\u001b[1;32m    349\u001b[0m         op_address\u001b[39m.\u001b[39mscope_in_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m         tm_comparators\u001b[39m=\u001b[39mtm_comparators,\n\u001b[1;32m    353\u001b[0m     )\n\u001b[0;32m--> 354\u001b[0m     node_candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_find_nodes_with_matching_context_and_inputs(op_exec_context)\n\u001b[1;32m    355\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m node_candidates:\n\u001b[1;32m    356\u001b[0m         node_candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_find_nodes_with_matching_context_among_inputless(op_exec_context)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/graph.py:280\u001b[0m, in \u001b[0;36mDefaultScopeNodeMatcher._find_nodes_with_matching_context_and_inputs\u001b[0;34m(self, op_exec_context)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    279\u001b[0m creator_id \u001b[39m=\u001b[39m info\u001b[39m.\u001b[39mcreator_id\n\u001b[0;32m--> 280\u001b[0m \u001b[39mfor\u001b[39;00m successor_node_key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nx_graph\u001b[39m.\u001b[39msuccessors(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_node_id_to_key_dict[creator_id]):\n\u001b[1;32m    281\u001b[0m     successor_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nx_graph\u001b[39m.\u001b[39mnodes[successor_node_key]\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m op_exec_context\u001b[39m.\u001b[39mmatches_saved_inputs_from(successor_node[DynamicGraph\u001b[39m.\u001b[39mOP_EXEC_CONTEXT_NODE_ATTR]):\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "from nncf.torch.dynamic_graph.io_handling import wrap_nncf_model_inputs_with_objwalk\n",
    "from nncf.torch.dynamic_graph.io_handling import wrap_nncf_model_outputs_with_objwalk\n",
    "from nncf.torch.dynamic_graph.context import no_nncf_trace\n",
    "from nncf.torch.nested_objects_traversal import objwalk\n",
    "from nncf.torch.utils import is_tensor\n",
    "from nncf.torch.dynamic_graph.io_handling import replicate_same_tensors\n",
    "from nncf.torch.utils import get_model_device\n",
    "\n",
    "def wrap_inputs(args, kwargs):\n",
    "        return wrap_nncf_model_inputs_with_objwalk(args, kwargs)\n",
    "\n",
    "def wrap_outputs(retval):\n",
    "    return wrap_nncf_model_outputs_with_objwalk(retval)\n",
    "\n",
    "def create_dummy_forward_fn(data_loader, device):\n",
    "    def dummy_forward(model):\n",
    "        with no_nncf_trace():\n",
    "            data_item = next(iter(data_loader))\n",
    "            args, kwargs = data_loader.get_inputs(data_item)\n",
    "\n",
    "            def send_to_device(tensor):\n",
    "                return tensor.to(device)\n",
    "\n",
    "            args = objwalk(args, is_tensor, send_to_device)\n",
    "            kwargs = objwalk(kwargs, is_tensor, send_to_device)\n",
    "\n",
    "        args, kwargs = wrap_inputs(args, kwargs)\n",
    "        retval = model(*args, **kwargs)\n",
    "        retval = replicate_same_tensors(retval)\n",
    "        return wrap_outputs(retval)\n",
    "\n",
    "    return dummy_forward\n",
    "\n",
    "dummy_forward_fn = create_dummy_forward_fn(text_encoder_dataloader, get_model_device(model))\n",
    "\n",
    "text_controller, text_encoder = create_compressed_model(\n",
    "    model.transformer, \n",
    "    text_encoder_config, \n",
    "    dummy_forward_fn=dummy_forward_fn,\n",
    "    wrap_inputs_fn=wrap_inputs,\n",
    "    wrap_outputs_fn=wrap_outputs,)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomeov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
