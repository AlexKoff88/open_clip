{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: you should run `openvino_benchmark.ipynb` notebook first that will generate `openvino_fp32` folder with the exported models that are used for quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"ViT-B-16-plus-240\"\n",
    "pretrained = \"laion400m_e32\"\n",
    "model, train_transform, eval_transform = open_clip.create_model_and_transforms(name, pretrained=pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "source_dir = \"openvino_fp32\"\n",
    "ouptut_dir = \"openvino_int8\"\n",
    "if not os.path.exists(ouptut_dir):\n",
    "    os.makedirs(ouptut_dir)\n",
    "with open(Path(ouptut_dir) / \"model_index.txt\", 'w') as fd:\n",
    "    fd.write(f\"{name},{pretrained}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = open_clip.get_tokenizer('ViT-B-16-plus-240')\n",
    "\n",
    "image = eval_transform(Image.open(\"../../docs/CLIP.png\")).unsqueeze(0)\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def get_pil_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "def check_text_data(data):\n",
    "    if isinstance(data, str):\n",
    "        return True\n",
    "    if isinstance(data, list):\n",
    "        return all(isinstance(x, str) for x in data)\n",
    "    return False    \n",
    "\n",
    "def laion2B_preprocess_train(examples, train_transforms, tokenize_captions, image_column=\"url\", text_column=\"caption\"):\n",
    "    url = examples[image_column]\n",
    "    try:\n",
    "        image = get_pil_from_url(url)\n",
    "        if not check_text_data(examples[text_column]):\n",
    "            raise ValueError(\"Text data is not valid\")\n",
    "    except Exception:\n",
    "        print(f\"Can't load image from url: {url}\")\n",
    "        return None\n",
    "\n",
    "    examples[\"pixel_values\"] = train_transforms(image)\n",
    "    examples[\"text\"] = tokenize_captions(examples)\n",
    "    return examples\n",
    "\n",
    "def tokenize_captions(examples, is_train=True):\n",
    "    caption_column = \"caption\"\n",
    "    captions = []\n",
    "    caption = examples[caption_column]\n",
    "    if isinstance(caption, str):\n",
    "        captions.append(caption)\n",
    "    elif isinstance(caption, (list, np.ndarray)):\n",
    "        # take a random caption if there are multiple\n",
    "        captions.append(random.choice(caption) if is_train else caption[0])\n",
    "    else:\n",
    "        raise ValueError(f\"Caption column `{caption_column}` should contain either strings or lists of strings.\")\n",
    "    input_ids = tokenizer(captions[0])[0]\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "max_train_samples = 10000\n",
    "dataset = load_dataset(\"laion/laion400m\", streaming=True)\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42, buffer_size=max_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_dtype = model.transformer.get_cast_dtype()\n",
    "\n",
    "def collate_fn(examples):\n",
    "    examples = [laion2B_preprocess_train(example, train_transform, tokenize_captions) for example in examples]\n",
    "    if not any(examples):\n",
    "        return None\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    \n",
    "    input_ids = torch.stack([example[\"text\"] for example in examples])\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def prepare_calibration_data(dataloader, init_steps):\n",
    "    data = []\n",
    "\n",
    "    print(f\"Fetching {init_steps} for the initialization...\")\n",
    "    for _, batch in tqdm(zip(range(init_steps), itertools.islice(dataloader, 0, init_steps))):\n",
    "        with torch.no_grad():\n",
    "            # Convert images to latent space\n",
    "            if batch:\n",
    "                data.append(\n",
    "                    (\n",
    "                        batch[\"pixel_values\"].to(\"cpu\"),\n",
    "                        batch[\"input_ids\"].to(\"cpu\")\n",
    "                    )\n",
    "                )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataloader_num_workers = 4\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, collate_fn=collate_fn, batch_size=batch_size, num_workers=dataloader_num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_init_steps = 500\n",
    "calibration_data = prepare_calibration_data(dataloader, opt_init_steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nncf\n",
    "from nncf.quantization.advanced_parameters  import AdvancedQuantizationParameters, AdvancedBiasCorrectionParameters\n",
    "from nncf.scopes import IgnoredScope\n",
    "\n",
    "ignored_scope = IgnoredScope(\n",
    "    types = [\"Divide\"]\n",
    ")\n",
    "\n",
    "\n",
    "advanced_parameters = AdvancedQuantizationParameters(\n",
    "    backend_params = {'use_pot': True}, \n",
    "    bias_correction_params = AdvancedBiasCorrectionParameters(apply_for_all_nodes=True, threshold=float('inf')),\n",
    "    overflow_fix=\"disable\")\n",
    "\n",
    "def quantize_image_encoder(model, data_loader):\n",
    "    quantization_dataset = nncf.Dataset(data_loader, lambda x: x[0])\n",
    "\n",
    "    quantized_model = nncf.quantize(\n",
    "                            model,\n",
    "                            quantization_dataset,\n",
    "                            model_type=nncf.ModelType.TRANSFORMER,\n",
    "                            preset=nncf.QuantizationPreset.MIXED,\n",
    "                            fast_bias_correction=False,\n",
    "                            subset_size=opt_init_steps,\n",
    "                            advanced_parameters=advanced_parameters,\n",
    "                            ignored_scope=ignored_scope,\n",
    "                            )\n",
    "    return quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino.runtime as ov\n",
    "from pathlib import Path\n",
    "\n",
    "ov_model_path = Path(source_dir) / \"image_encoder.xml\"\n",
    "\n",
    "core = ov.Core()\n",
    "image_encoder = core.read_model(ov_model_path)\n",
    "\n",
    "q_image_encoder = quantize_image_encoder(image_encoder, calibration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov.serialize(q_image_encoder, ouptut_dir + \"/image_encoder.xml\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_parameters = AdvancedQuantizationParameters(\n",
    "    backend_params = {'use_pot': True}, \n",
    "    bias_correction_params = AdvancedBiasCorrectionParameters(apply_for_all_nodes=True, threshold=float('inf')),\n",
    "    overflow_fix=\"disable\"\n",
    "    )\n",
    "\n",
    "\n",
    "def quantize_text_encoder(model, data_loader):\n",
    "    quantization_dataset = nncf.Dataset(data_loader, lambda x: x[1])\n",
    "\n",
    "    quantized_model = nncf.quantize(\n",
    "                            model,\n",
    "                            quantization_dataset,\n",
    "                            model_type=nncf.ModelType.TRANSFORMER,\n",
    "                            preset=nncf.QuantizationPreset.MIXED,\n",
    "                            fast_bias_correction=False,\n",
    "                            subset_size=opt_init_steps,\n",
    "                            advanced_parameters=advanced_parameters,\n",
    "                            )\n",
    "    return quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model_path = Path(source_dir) / \"text_encoder.xml\"\n",
    "\n",
    "core = ov.Core()\n",
    "text_encoder = core.read_model(ov_model_path)\n",
    "\n",
    "q_text_encoder = quantize_text_encoder(text_encoder, calibration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov.serialize(q_text_encoder, ouptut_dir + \"/text_encoder.xml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomeov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
