{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"ViT-B-16-plus-240\"\n",
    "pretrained = \"laion400m_e32\"\n",
    "model, train_transform, eval_transform = open_clip.create_model_and_transforms(name, pretrained=pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ouptut_dir = \"openvino_fp32\"\n",
    "if not os.path.exists(ouptut_dir):\n",
    "    os.makedirs(ouptut_dir)\n",
    "    \n",
    "with open(Path(ouptut_dir) / \"model_index.txt\", 'w') as fd:\n",
    "    fd.write(f\"{name},{pretrained}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = open_clip.get_tokenizer('ViT-B-16-plus-240')\n",
    "\n",
    "image = eval_transform(Image.open(\"../docs/CLIP.png\")).unsqueeze(0)\n",
    "text = tokenizer(\"a cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model.visual,\n",
    "    image,\n",
    "    \"image_encoder.onnx\",  # where to save the model\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=[\"image\"],  # the model's input names\n",
    "    output_names=[\"image_embedding\"],  # the model's output names\n",
    "    dynamic_axes={  # variable length axes\n",
    "        \"image\": {0: \"batch\"},\n",
    "        \"image_embedding\": {0: \"batch\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools.mo import convert_model\n",
    "import openvino.runtime as ov\n",
    "\n",
    "ov_encoder = convert_model(\"image_encoder.onnx\")\n",
    "ov.serialize(ov_encoder, ouptut_dir + \"/image_encoder.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformerExportWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.model.token_embedding(text)\n",
    "        x = x + self.model.positional_embedding\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.model.transformer(x, attn_mask=self.model.attn_mask)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.model.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.model.text_projection\n",
    "        return x\n",
    "\n",
    "transformer_export_wrapper = TextTransformerExportWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    transformer_export_wrapper,\n",
    "    text,\n",
    "    \"text_encoder.onnx\",  # where to save the model\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=[\"input_ids\"],  # the model's input names\n",
    "    output_names=[\"text_embeds\"],  # the model's output names\n",
    "    dynamic_axes={  # variable length axes\n",
    "        \"input_ids\": {0: \"batch\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools.mo import convert_model\n",
    "import openvino.runtime as ov\n",
    "\n",
    "ov_transformer = convert_model(\"text_encoder.onnx\")\n",
    "ov.serialize(ov_transformer, ouptut_dir + \"/text_encoder.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m image_encoder.onnx -shape \"image[1,3,240,240]\" -api sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m text_encoder.onnx -shape \"input_ids[1,77]\" -api sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    (image, text),\n",
    "    \"model.onnx\",  # where to save the model\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=[\"image\", \"text\"],  # the model's input names\n",
    "    output_names=[\"image_embedding\"],  # the model's output names\n",
    "    dynamic_axes={  # variable length axes\n",
    "        \"image\": {0: \"batch\"},\n",
    "        \"text\": {0: \"batch\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools.mo import convert_model\n",
    "import openvino.runtime as ov\n",
    "\n",
    "ov_transformer = convert_model(\"model.onnx\")\n",
    "ov.serialize(ov_transformer, \"model.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m model.xml -shape \"image[1,3,240,240],text[1,77]\" -api sync"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomeov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
