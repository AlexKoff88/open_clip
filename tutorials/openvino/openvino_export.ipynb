{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"ViT-B-16\" #-plus-240\n",
    "pretrained = \"laion400m_e32\"\n",
    "model, train_transform, eval_transform = open_clip.create_model_and_transforms(name, pretrained=pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ouptut_dir = \"openvino_fp32\"\n",
    "if not os.path.exists(ouptut_dir):\n",
    "    os.makedirs(ouptut_dir)\n",
    "    \n",
    "with open(Path(ouptut_dir) / \"model_index.txt\", 'w') as fd:\n",
    "    fd.write(f\"{name},{pretrained}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = open_clip.get_tokenizer(name)\n",
    "\n",
    "image = eval_transform(Image.open(\"../../docs/CLIP.png\")).unsqueeze(0)\n",
    "text = tokenizer(\"a cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model.visual,\n",
    "    image,\n",
    "    \"image_encoder.onnx\",  # where to save the model\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=[\"image\"],  # the model's input names\n",
    "    output_names=[\"image_embedding\"],  # the model's output names\n",
    "    dynamic_axes={  # variable length axes\n",
    "        \"image\": {0: \"batch\"},\n",
    "        \"image_embedding\": {0: \"batch\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools.mo import convert_model\n",
    "import openvino.runtime as ov\n",
    "\n",
    "ov_encoder = convert_model(\"image_encoder.onnx\")\n",
    "ov.serialize(ov_encoder, ouptut_dir + \"/image_encoder.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformerExportWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.model.token_embedding(text)\n",
    "        x = x + self.model.positional_embedding\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.model.transformer(x, attn_mask=self.model.attn_mask)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.model.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.model.text_projection\n",
    "        return x\n",
    "\n",
    "transformer_export_wrapper = TextTransformerExportWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/torch/onnx/symbolic_opset9.py:5408: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    transformer_export_wrapper,\n",
    "    text,\n",
    "    \"text_encoder.onnx\",  # where to save the model\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=[\"input_ids\"],  # the model's input names\n",
    "    output_names=[\"text_embeds\"],  # the model's output names\n",
    "    dynamic_axes={  # variable length axes\n",
    "        \"input_ids\": {0: \"batch\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to send event with error cannot schedule new futures after shutdown.\n",
      "WARNING:root:Failed to send event with error cannot schedule new futures after shutdown.\n"
     ]
    }
   ],
   "source": [
    "from openvino.tools.mo import convert_model\n",
    "import openvino.runtime as ov\n",
    "\n",
    "ov_transformer = convert_model(\"text_encoder.onnx\")\n",
    "ov.serialize(ov_transformer, ouptut_dir + \"/text_encoder.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2023.0.0-10862-40bf400b189\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2023.0.0-10862-40bf400b189\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to PerformanceMode.LATENCY.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 380.48 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     image (node: image) : f32 / [...] / [?,3,224,224]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     image_embedding (node: image_embedding) : f32 / [...] / [?,512]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'image': [1,3,240,240]\n",
      "[ ERROR ] Check 'T::broadcast_merge_into(output_shape, input_shapes[1], autob)' failed at src/core/shape_inference/include/utils.hpp:42:\n",
      "While validating node 'opset1::Add /Add_1 (/Concat_2[0]:f32[1,226,768], onnx::Add_1818[0]:f32[197,768]) -> (f32[?,197,768])' with friendly_name '/Add_1':\n",
      "Argument shapes are inconsistent.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/openvino/tools/benchmark/main.py\", line 390, in main\n",
      "    model.reshape(shapes)\n",
      "RuntimeError: Check 'T::broadcast_merge_into(output_shape, input_shapes[1], autob)' failed at src/core/shape_inference/include/utils.hpp:42:\n",
      "While validating node 'opset1::Add /Add_1 (/Concat_2[0]:f32[1,226,768], onnx::Add_1818[0]:f32[197,768]) -> (f32[?,197,768])' with friendly_name '/Add_1':\n",
      "Argument shapes are inconsistent.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m image_encoder.onnx -shape \"image[1,3,240,240]\" -api sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2023.0.0-10862-40bf400b189\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2023.0.0-10862-40bf400b189\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to PerformanceMode.LATENCY.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 373.38 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [?,77]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     text_embeds (node: text_embeds) : f32 / [...] / [?,512]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'input_ids': [1,77]\n",
      "[ INFO ] Reshape model took 48.29 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [1,77]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     text_embeds (node: text_embeds) : f32 / [...] / [1,512]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 888.38 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1\n",
      "[ INFO ]   NUM_STREAMS: 1\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 18\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.LATENCY\n",
      "[ INFO ]   EXECUTION_MODE_HINT: ExecutionMode.PERFORMANCE\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[ INFO ]   ENABLE_CPU_PINNING: True\n",
      "[ INFO ]   SCHEDULING_CORE_TYPE: SchedulingCoreType.ANY_CORE\n",
      "[ INFO ]   ENABLE_HYPER_THREADING: True\n",
      "[ INFO ]   EXECUTION_DEVICES: ['CPU']\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input_ids'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input_ids' with random values \n",
      "[Step 10/11] Measuring performance (Start inference synchronously, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 13.61 ms\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alex/virt_envs/tomeov/bin/benchmark_app\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/openvino/tools/benchmark/main.py\", line 570, in main\n",
      "    fps, median_latency_ms, avg_latency_ms, min_latency_ms, max_latency_ms, total_duration_sec, iteration = benchmark.main_loop(requests, data_queue, batch_size, args.latency_percentile, pcseq)\n",
      "  File \"/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/openvino/tools/benchmark/benchmark.py\", line 167, in main_loop\n",
      "    times, total_duration_sec, iteration = self.sync_inference(requests[0], data_queue)\n",
      "  File \"/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/openvino/tools/benchmark/benchmark.py\", line 98, in sync_inference\n",
      "    request.infer()\n",
      "  File \"/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/openvino/runtime/ie_api.py\", line 74, in infer\n",
      "    return OVDict(super().infer(_data_dispatch(\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m text_encoder.onnx -shape \"input_ids[1,77]\" -api sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    (image, text),\n",
    "    \"model.onnx\",  # where to save the model\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=[\"image\", \"text\"],  # the model's input names\n",
    "    output_names=[\"image_embedding\"],  # the model's output names\n",
    "    dynamic_axes={  # variable length axes\n",
    "        \"image\": {0: \"batch\"},\n",
    "        \"text\": {0: \"batch\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools.mo import convert_model\n",
    "import openvino.runtime as ov\n",
    "\n",
    "ov_transformer = convert_model(\"model.onnx\")\n",
    "ov.serialize(ov_transformer, \"model.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m model.xml -shape \"image[1,3,240,240],text[1,77]\" -api sync"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomeov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
