{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to use int8 operations with OpenCLIP.\n",
    "\n",
    "Usually matrix multiplies are conducted in float16 or bfloat16, but int8 operations are faster.\n",
    "\n",
    "For more information please see https://github.com/mlfoundations/open_clip#int8-support\n",
    "\n",
    "We ran this on an A100 GPU\n",
    "\n",
    "Note that this tutorial requires two additional pip installs on top of those required for standard OpenCLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary installs for int8\n",
    "%pip install scikit-image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with using a standard OpenCLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import open_clip\n",
    "from open_clip import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "name = \"ViT-B-16-plus-240\"\n",
    "pretrained = \"laion400m_e32\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(name, pretrained=pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check out the example image we will be using for classification\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data, data_dir\n",
    "import os\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "img = data.astronaut()\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preprocess image and text\n",
    "img = Image.open(os.path.join(data_dir, 'astronaut.png')).convert(\"RGB\")\n",
    "img_preprocessed = preprocess(img).unsqueeze(0)\n",
    "\n",
    "descriptions = {\n",
    "    \"page\": \"a page of text about segmentation\",\n",
    "    \"chelsea\": \"a facial photo of a tabby cat\",\n",
    "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
    "    \"rocket\": \"a rocket standing on a launchpad\",\n",
    "    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n",
    "    \"camera\": \"a person looking at a camera on a tripod\",\n",
    "    \"horse\": \"a black-and-white silhouette of a horse\", \n",
    "    \"coffee\": \"a cup of coffee on a saucer\"\n",
    "}\n",
    "texts = descriptions.values()\n",
    "\n",
    "text_processed = tokenizer.tokenize(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions from the model\n",
    "with torch.cuda.amp.autocast():\n",
    "    img_embedding, text_embedding, _ = model(img_preprocessed, text_processed)\n",
    "probs = (100 * img_embedding @ text_embedding.T).softmax(dim=-1)\n",
    "plt.bar(descriptions.keys(), probs.squeeze().detach().cpu().numpy())\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Probability (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomeov\n",
    "from copy import deepcopy\n",
    "\n",
    "model_opt = deepcopy(model)\n",
    "\n",
    "tomeov.patch_openclip(model_opt, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with torch.cuda.amp.autocast():\n",
    "    start = time.time()\n",
    "    img_embedding, text_embedding, _ = model_opt(img_preprocessed, text_processed)\n",
    "    print(f\"elapsed: {time.time() - start} seconds\")\n",
    "probs = (100 * img_embedding @ text_embedding.T).softmax(dim=-1)\n",
    "plt.bar(descriptions.keys(), probs.squeeze().detach().cpu().numpy())\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Probability (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ouptut_dir = \"openvino_tome\"\n",
    "if not os.path.exists(ouptut_dir):\n",
    "    os.makedirs(ouptut_dir)\n",
    "    \n",
    "with open(Path(ouptut_dir) / \"model_index.txt\", 'w') as fd:\n",
    "    fd.write(f\"{name},{pretrained}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = open_clip.get_tokenizer(name)\n",
    "\n",
    "image = preprocess(Image.open(\"../../docs/CLIP.png\")).unsqueeze(0)\n",
    "text = tokenizer(\"a cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model_opt.visual,\n",
    "    image,\n",
    "    \"image_encoder.onnx\",  # where to save the model\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=[\"image\"],  # the model's input names\n",
    "    output_names=[\"image_embedding\"],  # the model's output names\n",
    "    dynamic_axes={  # variable length axes\n",
    "        \"image\": {0: \"batch\"},\n",
    "        \"image_embedding\": {0: \"batch\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools.mo import convert_model\n",
    "import openvino.runtime as ov\n",
    "\n",
    "ov_encoder = convert_model(\"image_encoder.onnx\")\n",
    "opt_image_encoder_path = ouptut_dir + \"/image_encoder.xml\"\n",
    "ov.serialize(ov_encoder, opt_image_encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformerExportWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.model.token_embedding(text)\n",
    "        x = x + self.model.positional_embedding\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.model.transformer(x, attn_mask=self.model.attn_mask)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.model.ln_final(x)  # [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.model.text_projection\n",
    "        return x\n",
    "\n",
    "transformer_export_wrapper = TextTransformerExportWrapper(model_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    transformer_export_wrapper,\n",
    "    text,\n",
    "    \"text_encoder.onnx\",  # where to save the model\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=[\"input_ids\"],  # the model's input names\n",
    "    output_names=[\"text_embeds\"],  # the model's output names\n",
    "    dynamic_axes={  # variable length axes\n",
    "        \"input_ids\": {0: \"batch\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools.mo import convert_model\n",
    "import openvino.runtime as ov\n",
    "\n",
    "ov_transformer = convert_model(\"text_encoder.onnx\")\n",
    "ov.serialize(ov_transformer, ouptut_dir + \"/text_encoder.xml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
