{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/openvino/offline_transformations/__init__.py:10: FutureWarning: The module is private and following namespace `offline_transformations` will be removed in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nncf import NNCFConfig\n",
    "from nncf.common.logging import nncf_logger\n",
    "from nncf.torch import create_compressed_model, register_default_init_args\n",
    "from nncf.torch.initialization import PTInitializingDataLoader\n",
    "from nncf.torch.layer_utils import CompressionParameter\n",
    "\n",
    "from PIL import Image\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_transform, eval_transform = open_clip.create_model_and_transforms('ViT-B-16-plus-240', pretrained='laion400m_e32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = open_clip.get_tokenizer('ViT-B-16-plus-240')\n",
    "\n",
    "image = eval_transform(Image.open(\"../docs/CLIP.png\")).unsqueeze(0)\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def get_pil_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "BACKUP_PAIR = (\n",
    "    get_pil_from_url(\n",
    "        \"https://thumbs.dreamstime.com/t/altai-mountains-mountain-lake-russia-siberia-chuya-ridge-49130812.jpg\"\n",
    "    ),\n",
    "    \"Altai mountains Stock Photography\",\n",
    ")\n",
    "AVAILABLE_EXAMPLES = []\n",
    "\n",
    "def check_text_data(data):\n",
    "    if isinstance(data, str):\n",
    "        return True\n",
    "    if isinstance(data, list):\n",
    "        return all(isinstance(x, str) for x in data)\n",
    "    return False    \n",
    "\n",
    "def laion2B_preprocess_train(examples, train_transforms, tokenize_captions, image_column=\"url\", text_column=\"caption\"):\n",
    "    url = examples[image_column]\n",
    "    try:\n",
    "        image = get_pil_from_url(url)\n",
    "        if not check_text_data(examples[text_column]):\n",
    "            raise ValueError(\"Text data is not valid\")\n",
    "        AVAILABLE_EXAMPLES.append((url, examples[text_column]))\n",
    "    except Exception:\n",
    "        print(f\"Can't load image from url: {url}, using cache with size: {len(AVAILABLE_EXAMPLES)}\")\n",
    "        if len(AVAILABLE_EXAMPLES) > 0:\n",
    "            backup_id = random.randint(0, len(AVAILABLE_EXAMPLES) - 1)\n",
    "            backup_example = AVAILABLE_EXAMPLES[backup_id]\n",
    "            try:\n",
    "                image = get_pil_from_url(backup_example[0])\n",
    "                examples[text_column] = backup_example[1]\n",
    "            except Exception:\n",
    "                print(f\"Can't load image from cached url: {backup_example[0]}, using backup\")\n",
    "                image = BACKUP_PAIR[0].copy()\n",
    "                examples[text_column] = BACKUP_PAIR[1]\n",
    "        else:\n",
    "            print(f\"Can't load image from url: {url}, using backup\")\n",
    "            image = BACKUP_PAIR[0].copy()\n",
    "            examples[text_column] = BACKUP_PAIR[1]\n",
    "\n",
    "    examples[\"pixel_values\"] = train_transforms(image)\n",
    "    examples[\"input_ids\"] = tokenize_captions(examples)\n",
    "    return examples\n",
    "\n",
    "def tokenize_captions(examples, is_train=True):\n",
    "    caption_column = \"caption\"\n",
    "    captions = []\n",
    "    caption = examples[caption_column]\n",
    "    if isinstance(caption, str):\n",
    "        captions.append(caption)\n",
    "    elif isinstance(caption, (list, np.ndarray)):\n",
    "        # take a random caption if there are multiple\n",
    "        captions.append(random.choice(caption) if is_train else caption[0])\n",
    "    else:\n",
    "        raise ValueError(f\"Caption column `{caption_column}` should contain either strings or lists of strings.\")\n",
    "    #inputs = tokenizer(captions[0], max_length=tokenizer.model_max_length, padding=\"do_not_pad\", truncation=True)\n",
    "    #input_ids = inputs.input_ids\n",
    "    input_ids = tokenizer(captions[0])[0]\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "max_train_samples = 10000\n",
    "dataset = load_dataset(\"laion/laion400m\", streaming=True)\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42, buffer_size=max_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_image(examples):\n",
    "    examples = [laion2B_preprocess_train(example, train_transform, tokenize_captions) for example in examples]\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        # \"attention_mask\": padded_tokens.attention_mask,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def prepare_nncf_init_data(dataloader, init_steps):\n",
    "    nncf_init_data = []\n",
    "\n",
    "    print(f\"Fetching {init_steps} for the initialization...\")\n",
    "    for _, batch in tqdm(zip(range(init_steps), itertools.islice(dataloader, 0, init_steps))):\n",
    "        with torch.no_grad():\n",
    "            # Convert images to latent space\n",
    "            \n",
    "            nncf_init_data.append(\n",
    "                (\n",
    "                    batch[\"pixel_values\"].to(\"cpu\"),\n",
    "                    batch[\"input_ids\"].to(\"cpu\")\n",
    "                )\n",
    "            )\n",
    "    return nncf_init_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 1\n",
    "dataloader_num_workers = 4\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, collate_fn=collate_fn_image, batch_size=train_batch_size, num_workers=dataloader_num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 100 for the initialization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33742f42aed42e48f1556d0aa08256e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't load image from url: https://s3.amazonaws.com/whataspace/space_pictures/pictures/000/033/180/fullwidth/_DSC0146.jpg?1591610094, using cache with size: 0\n",
      "Can't load image from url: https://s3.amazonaws.com/whataspace/space_pictures/pictures/000/033/180/fullwidth/_DSC0146.jpg?1591610094, using backup\n",
      "Can't load image from url: https://i0.wp.com/des.gearbest.com/uploads/pdm-desc-pic/Electronic/image/2016/11/25/1480056293210706.jpg?w=960, using cache with size: 2\n",
      "Can't load image from url: https://static.paraforme.fr/images/products/juvamine-concentre-chrome.jpg, using cache with size: 2\n",
      "Can't load image from url: http://cdn3.static-homes.com/cgi-bin/readimage/9eb0ae4fa92b5dfd09b03c9e3dc997c5_1_resizeto_193x143x1, using cache with size: 3\n",
      "Can't load image from url: https://img.shellporn.com/spcs/thumbs/155/299_hotel_wang_.jpg, using cache with size: 5\n",
      "Can't load image from url: https://sslh.ulximg.com/image/740x493/cover/1533483761_ac8b9fbc89ca57a7269ec0c7f5947094.jpg/2143af354a3c53d14969369d2c6cbc04/1533483761_cc5f2e1e234c809fa4408488e0b19e4d.jpg, using cache with size: 6\n",
      "Can't load image from url: http://resizing.flixster.com/wp7S_BA23xLXC2iPn_Ozyuck-m8=/320x455/dkpu1ddg7pbsk.cloudfront.net/movie/26/93/269324_ori.jpg, using cache with size: 7\n",
      "Can't load image from url: http://dyn1.heritagestatic.com/lf?set=path%5B9%2F5%2F5%2F4%2F9554311%5D%2Csizedata%5B220x350%5D&call=url%5Bfile%3Aproduct.chain%5D, using cache with size: 8\n",
      "Can't load image from url: https://www.picclickimg.com/d/l400/pict/123866029585_/Ancient-Roman-Mosaic-Double-Face-Glass-Pendant-Bead.jpg, using cache with size: 8\n",
      "Can't load image from url: http://thehaengallery.com/wp-content/uploads/EasyRotatorStorage/user-content/erc_27_1368152757/content/assets/xfs_600x550_s80_xfs_600x550_s80_Grand_Central_Station%20Windows-0.jpg, using cache with size: 10\n",
      "Can't load image from url: https://cdn.shopify.com/s/files/1/1162/0686/products/10191845_J8083IvoryBack.jpg?v=1552659023, using cache with size: 13\n",
      "Can't load image from url: https://donthavetime-73c7.kxcdn.com/wp-content/uploads/2018/03/Depositphotos_180879716_original-1024x683.jpg, using cache with size: 13\n",
      "Can't load image from url: https://img.lxrco.com/1739MQ359/1560272-gucci-guccissima-joy-hobo-bag-guccissima-cream-leather-shoulder-bags-t6vinp0nqf.large.jpg, using cache with size: 13\n",
      "Can't load image from url: http://i0.wp.com/www.ewindandsolar.com/i/2015/06/hy-380-led-puck-lights-for-cozy-living-room-design-led-puck-light-kit-under-cabinet-led-puck-lights-12v-led-puck-lights-led-puck-lights-120v-battery-powered-led-puck-lights-battery-led.jpg?w=200&strip=all, using cache with size: 18\n",
      "Can't load image from url: https://cdn1-www.momtastic.com/assets/uploads/2014/01/old-DIYs-e1389240447729.jpg, using cache with size: 22\n",
      "Can't load image from url: https://www.belleviemedical.com/wp-content/uploads/2016/03/Belle-Vie-Home-Face.png, using cache with size: 18\n",
      "Can't load image from url: http://s.en.fishki.net/upload/en/201211/23/2706/9840483e0be437697bcdb6501908d666.jpg, using cache with size: 19\n",
      "Can't load image from url: https://d2d00szk9na1qq.cloudfront.net/Product/0d387c2a-4687-4900-8861-17a0df831812/Images/Medium_0393423.jpg, using cache with size: 23\n"
     ]
    }
   ],
   "source": [
    "opt_init_steps = 100\n",
    "init_data = prepare_nncf_init_data(train_dataloader, opt_init_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.init_data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.init_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.init_data[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply NNCF optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder_config = {\n",
    "    \"input_info\": [\n",
    "        {  \n",
    "            \"sample_size\": [1, 3, 240, 240]\n",
    "        },\n",
    "    ],\n",
    "    \"log_dir\": \"./\",  # The log directory for NNCF-specific logging outputs.\n",
    "    \"compression\": [\n",
    "        {\n",
    "            \"algorithm\": \"quantization\",  # Specify the algorithm here.\n",
    "            \"preset\": \"mixed\",\n",
    "            \"initializer\": {\n",
    "                \"range\": {\"num_init_samples\": opt_init_steps},\n",
    "                \"batchnorm_adaptation\": {\"num_bn_adaptation_samples\": opt_init_steps},\n",
    "            },\n",
    "            \"scope_overrides\": {\"activations\": {\"{re}.*baddbmm_0\": {\"mode\": \"symmetric\"}}},\n",
    "            \"ignored_scopes\": [\n",
    "                \"{re}.*__add___[0-2]\",\n",
    "                \"{re}.*layer_norm_0\",\n",
    "                \"{re}.*Attention.*/bmm_0\",\n",
    "                \"{re}.*__truediv__*\",\n",
    "                \"{re}.*group_norm_0\",\n",
    "                \"{re}.*mul___[0-2]\",\n",
    "                \"{re}.*silu_[0-2]\",\n",
    "            ],\n",
    "            \"export_to_onnx_standard_ops\": True,\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "init_dataloader = torch.utils.data.DataLoader(InitDataset(init_data), batch_size=1, num_workers=1)\n",
    "\n",
    "class UnetInitDataLoader(PTInitializingDataLoader):\n",
    "    def get_inputs(self, dataloader_output):\n",
    "        image = dataloader_output[\"pixel_values\"].float().to(model.device, non_blocking=True)\n",
    "        return (image), {}\n",
    "\n",
    "    def get_target(self, dataloader_output):\n",
    "        return dataloader_output[\"pixel_values\"]\n",
    "\n",
    "nncf_config = NNCFConfig.from_dict(image_encoder_config)\n",
    "nncf_config = register_default_init_args(nncf_config, UnetInitDataLoader(init_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m compression_controller, image_encoder \u001b[39m=\u001b[39m create_compressed_model(model\u001b[39m.\u001b[39;49mvisual, nncf_config)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/telemetry/decorator.py:71\u001b[0m, in \u001b[0;36mtracked_function.__call__.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m event \u001b[39min\u001b[39;00m events:\n\u001b[1;32m     64\u001b[0m         telemetry\u001b[39m.\u001b[39msend_event(\n\u001b[1;32m     65\u001b[0m             event_category\u001b[39m=\u001b[39mcategory,\n\u001b[1;32m     66\u001b[0m             event_action\u001b[39m=\u001b[39mevent\u001b[39m.\u001b[39mname,\n\u001b[1;32m     67\u001b[0m             event_label\u001b[39m=\u001b[39mevent\u001b[39m.\u001b[39mdata,\n\u001b[1;32m     68\u001b[0m             event_value\u001b[39m=\u001b[39mevent\u001b[39m.\u001b[39mint_data,\n\u001b[1;32m     69\u001b[0m         )\n\u001b[0;32m---> 71\u001b[0m retval \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m category \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m category \u001b[39m!=\u001b[39m previous_category:\n\u001b[1;32m     74\u001b[0m     telemetry\u001b[39m.\u001b[39mend_session(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_category)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/model_creation.py:108\u001b[0m, in \u001b[0;36mcreate_compressed_model\u001b[0;34m(model, config, compression_state, dummy_forward_fn, wrap_inputs_fn, wrap_outputs_fn, dump_graphs)\u001b[0m\n\u001b[1;32m    104\u001b[0m maybe_convert_legacy_names_in_compress_state(compression_state)\n\u001b[1;32m    106\u001b[0m should_init \u001b[39m=\u001b[39m compression_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m nncf_network \u001b[39m=\u001b[39m create_nncf_network(model, config, dummy_forward_fn, wrap_inputs_fn, wrap_outputs_fn)\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m dump_graphs \u001b[39mand\u001b[39;00m is_main_process():\n\u001b[1;32m    111\u001b[0m     nncf_network\u001b[39m.\u001b[39mnncf\u001b[39m.\u001b[39mget_graph()\u001b[39m.\u001b[39mvisualize_graph(osp\u001b[39m.\u001b[39mjoin(config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlog_dir\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39moriginal_graph.dot\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/model_creation.py:201\u001b[0m, in \u001b[0;36mcreate_nncf_network\u001b[0;34m(model, config, dummy_forward_fn, wrap_inputs_fn, wrap_outputs_fn)\u001b[0m\n\u001b[1;32m    198\u001b[0m     ignored_scopes \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mignored_scopes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    199\u001b[0m     target_scopes \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtarget_scopes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 201\u001b[0m     nncf_network \u001b[39m=\u001b[39m NNCFNetwork(\n\u001b[1;32m    202\u001b[0m         model,\n\u001b[1;32m    203\u001b[0m         input_infos\u001b[39m=\u001b[39;49minput_info_list,\n\u001b[1;32m    204\u001b[0m         dummy_forward_fn\u001b[39m=\u001b[39;49mdummy_forward_fn,\n\u001b[1;32m    205\u001b[0m         wrap_inputs_fn\u001b[39m=\u001b[39;49mwrap_inputs_fn,\n\u001b[1;32m    206\u001b[0m         wrap_outputs_fn\u001b[39m=\u001b[39;49mwrap_outputs_fn,\n\u001b[1;32m    207\u001b[0m         ignored_scopes\u001b[39m=\u001b[39;49mignored_scopes,\n\u001b[1;32m    208\u001b[0m         target_scopes\u001b[39m=\u001b[39;49mtarget_scopes,\n\u001b[1;32m    209\u001b[0m         scopes_without_shape_matching\u001b[39m=\u001b[39;49mscopes_without_shape_matching,\n\u001b[1;32m    210\u001b[0m     )\n\u001b[1;32m    212\u001b[0m     nncf_network\u001b[39m.\u001b[39mnncf\u001b[39m.\u001b[39mget_tracing_context()\u001b[39m.\u001b[39mdisable_trace_dynamic_graph()\n\u001b[1;32m    214\u001b[0m synchronize_all_processes_in_distributed_mode()\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/nncf_network.py:769\u001b[0m, in \u001b[0;36mNNCFNetworkMeta.__call__\u001b[0;34m(cls, original_model, input_infos, dummy_forward_fn, wrap_inputs_fn, scopes_without_shape_matching, ignored_scopes, target_scopes, wrap_outputs_fn)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[39mThis function plays the role of a \"constructor\" call in the `nncf_network = NNCFNetwork(original_model, ...)`\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[39msyntax. *_scopes arguments are to be passed as string representation of either\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[39mchecks.\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    768\u001b[0m original_class \u001b[39m=\u001b[39m original_model\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[0;32m--> 769\u001b[0m original_model\u001b[39m.\u001b[39m_nncf \u001b[39m=\u001b[39m NNCFNetworkInterface(\n\u001b[1;32m    770\u001b[0m     original_model,\n\u001b[1;32m    771\u001b[0m     input_infos,\n\u001b[1;32m    772\u001b[0m     dummy_forward_fn,\n\u001b[1;32m    773\u001b[0m     wrap_inputs_fn,\n\u001b[1;32m    774\u001b[0m     scopes_without_shape_matching,\n\u001b[1;32m    775\u001b[0m     ignored_scopes,\n\u001b[1;32m    776\u001b[0m     target_scopes,\n\u001b[1;32m    777\u001b[0m     wrap_outputs_fn,\n\u001b[1;32m    778\u001b[0m )  \u001b[39m# pylint:disable=protected-access\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[39m# The new class will also have an adjusted metaclass to avoid a \"metaclass conflict\" upon\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39m# class creation\u001b[39;00m\n\u001b[1;32m    781\u001b[0m original_metaclass \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(original_model\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/nncf_network.py:255\u001b[0m, in \u001b[0;36mNNCFNetworkInterface.__init__\u001b[0;34m(self, model, input_infos, dummy_forward_fn, wrap_inputs_fn, scopes_without_shape_matching, ignored_scopes, target_scopes, wrap_outputs_fn)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insertions_into_original_graph \u001b[39m=\u001b[39m (\n\u001b[1;32m    248\u001b[0m     {}\n\u001b[1;32m    249\u001b[0m )  \u001b[39m# type: Dict[PTTargetPoint, List[Tuple[Callable, TransformationPriority]]]\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _orig_graph_build_forward_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_dummy_forward_fn_for_graph_building(\n\u001b[1;32m    252\u001b[0m     with_input_tracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, with_output_tracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    253\u001b[0m )\n\u001b[0;32m--> 255\u001b[0m eval_op_scopes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_collect_eval_op_scopes(model, _orig_graph_build_forward_fn)\n\u001b[1;32m    257\u001b[0m \u001b[39m# all modules called in eval mode should be replaced prior to graph building\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replace_modules_by_nncf_modules(model, device, eval_op_scopes)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/nncf_network.py:700\u001b[0m, in \u001b[0;36mNNCFNetworkInterface._collect_eval_op_scopes\u001b[0;34m(self, model, dummy_forward_fn)\u001b[0m\n\u001b[1;32m    698\u001b[0m tracer \u001b[39m=\u001b[39m GraphTracer(dummy_forward_fn)\n\u001b[1;32m    699\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 700\u001b[0m eval_graph \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39;49mtrace_graph(model, as_eval\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    701\u001b[0m root_scope \u001b[39m=\u001b[39m Scope()\n\u001b[1;32m    702\u001b[0m \u001b[39mfor\u001b[39;00m dyn_graph_node \u001b[39min\u001b[39;00m eval_graph\u001b[39m.\u001b[39mget_all_nodes():\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/graph_tracer.py:113\u001b[0m, in \u001b[0;36mGraphTracer.trace_graph\u001b[0;34m(self, model, context_to_use, as_eval)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m as_eval:\n\u001b[1;32m    112\u001b[0m     \u001b[39mwith\u001b[39;00m training_mode_switcher(model, is_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 113\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcustom_forward_fn(model)\n\u001b[1;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcustom_forward_fn(model)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/graph_tracer.py:155\u001b[0m, in \u001b[0;36mcreate_dummy_forward_fn.<locals>.default_dummy_forward_fn\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m         args, kwargs \u001b[39m=\u001b[39m wrap_inputs_fn(args, kwargs)\n\u001b[0;32m--> 155\u001b[0m retval \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m with_output_tracing:\n\u001b[1;32m    157\u001b[0m     retval \u001b[39m=\u001b[39m replicate_same_tensors(retval)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:151\u001b[0m, in \u001b[0;36mwrap_module_call.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         ctx\u001b[39m.\u001b[39min_skipped_block \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     retval \u001b[39m=\u001b[39m module_call(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m ITERATION_MODULES\u001b[39m.\u001b[39mregistry_dict:\n\u001b[1;32m    154\u001b[0m     ctx\u001b[39m.\u001b[39mreset_operator_call_count_in_scope(ctx\u001b[39m.\u001b[39mscope)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/experimental/open_clip/src/open_clip/transformer.py:486\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    483\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_pre(x)\n\u001b[1;32m    485\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m    487\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_pool \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:151\u001b[0m, in \u001b[0;36mwrap_module_call.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         ctx\u001b[39m.\u001b[39min_skipped_block \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     retval \u001b[39m=\u001b[39m module_call(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m ITERATION_MODULES\u001b[39m.\u001b[39mregistry_dict:\n\u001b[1;32m    154\u001b[0m     ctx\u001b[39m.\u001b[39mreset_operator_call_count_in_scope(ctx\u001b[39m.\u001b[39mscope)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/experimental/open_clip/src/open_clip/transformer.py:321\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    319\u001b[0m         x \u001b[39m=\u001b[39m checkpoint(r, x, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, attn_mask)\n\u001b[1;32m    320\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m         x \u001b[39m=\u001b[39m r(x, attn_mask\u001b[39m=\u001b[39;49mattn_mask)\n\u001b[1;32m    322\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:151\u001b[0m, in \u001b[0;36mwrap_module_call.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         ctx\u001b[39m.\u001b[39min_skipped_block \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     retval \u001b[39m=\u001b[39m module_call(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m ITERATION_MODULES\u001b[39m.\u001b[39mregistry_dict:\n\u001b[1;32m    154\u001b[0m     ctx\u001b[39m.\u001b[39mreset_operator_call_count_in_scope(ctx\u001b[39m.\u001b[39mscope)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/experimental/open_clip/src/open_clip/transformer.py:242\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    239\u001b[0m k_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1_kv(k_x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mln_1_kv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m k_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    240\u001b[0m v_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1_kv(v_x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mln_1_kv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m v_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m x \u001b[39m=\u001b[39m q_x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(q_x\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(q_x), k_x\u001b[39m=\u001b[39;49mk_x, v_x\u001b[39m=\u001b[39;49mv_x, attn_mask\u001b[39m=\u001b[39;49mattn_mask))\n\u001b[1;32m    243\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mls_2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x)))\n\u001b[1;32m    244\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/work/experimental/open_clip/src/open_clip/transformer.py:228\u001b[0m, in \u001b[0;36mResidualAttentionBlock.attention\u001b[0;34m(self, q_x, k_x, v_x, attn_mask)\u001b[0m\n\u001b[1;32m    225\u001b[0m v_x \u001b[39m=\u001b[39m v_x \u001b[39mif\u001b[39;00m v_x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m q_x\n\u001b[1;32m    227\u001b[0m attn_mask \u001b[39m=\u001b[39m attn_mask\u001b[39m.\u001b[39mto(q_x\u001b[39m.\u001b[39mdtype) \u001b[39mif\u001b[39;00m attn_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    229\u001b[0m     q_x, k_x, v_x, need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, attn_mask\u001b[39m=\u001b[39;49mattn_mask\n\u001b[1;32m    230\u001b[0m )[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:151\u001b[0m, in \u001b[0;36mwrap_module_call.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         ctx\u001b[39m.\u001b[39min_skipped_block \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     retval \u001b[39m=\u001b[39m module_call(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m ITERATION_MODULES\u001b[39m.\u001b[39mregistry_dict:\n\u001b[1;32m    154\u001b[0m     ctx\u001b[39m.\u001b[39mreset_operator_call_count_in_scope(ctx\u001b[39m.\u001b[39mscope)\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/torch/nn/modules/activation.py:1167\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1157\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1158\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[1;32m   1165\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[1;32m   1166\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1168\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1169\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1170\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1172\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1173\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1174\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[1;32m   1175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1176\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:94\u001b[0m, in \u001b[0;36mwrap_operator.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     result \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39mtensor_cache\n\u001b[1;32m     93\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     result \u001b[39m=\u001b[39m _execute_op(op_address, operator_info, operator, ctx, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     96\u001b[0m str_op_address \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(op_address)\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m str_op_address \u001b[39min\u001b[39;00m ctx\u001b[39m.\u001b[39mend_node_name_of_skipped_block:\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:191\u001b[0m, in \u001b[0;36m_execute_op\u001b[0;34m(op_address, operator_info, operator, ctx, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[39mif\u001b[39;00m is_debug() \u001b[39mand\u001b[39;00m node \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m         ctx\u001b[39m.\u001b[39mregister_node_call(node)\n\u001b[0;32m--> 191\u001b[0m result \u001b[39m=\u001b[39m trace_tensors(result, node, ctx)\n\u001b[1;32m    192\u001b[0m result \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39mexecute_post_hooks(op_address, result)\n\u001b[1;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/virt_envs/tomeov/lib/python3.8/site-packages/nncf/torch/dynamic_graph/trace_tensor.py:139\u001b[0m, in \u001b[0;36mtrace_tensors\u001b[0;34m(operator_output, node, ctx)\u001b[0m\n\u001b[1;32m    137\u001b[0m meta \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m node \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     meta \u001b[39m=\u001b[39m TensorMeta(node\u001b[39m.\u001b[39mnode_id, i, x\u001b[39m.\u001b[39;49mshape, get_dtype(x))\n\u001b[1;32m    140\u001b[0m tt \u001b[39m=\u001b[39m TracedTensor\u001b[39m.\u001b[39mfrom_torch_tensor(x, meta)\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m ctx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "compression_controller, image_encoder = create_compressed_model(model.visual, nncf_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomeov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
