{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"ViT-B-16-plus-240\"\n",
    "pretrained = \"laion400m_e32\"\n",
    "model, train_transform, eval_transform = open_clip.create_model_and_transforms(name, pretrained=pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"openvino_fp32\")\n",
    "with open(\"openvino_fp32/model_index.txt\", 'a') as fd:\n",
    "    fd.write(f\"{name}, {pretrained}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = open_clip.get_tokenizer('ViT-B-16-plus-240')\n",
    "\n",
    "image = eval_transform(Image.open(\"../docs/CLIP.png\")).unsqueeze(0)\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model.visual,\n",
    "    image,\n",
    "    \"image_encoder.onnx\",  # where to save the model\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=[\"image\"],  # the model's input names\n",
    "    output_names=[\"image_embedding\"],  # the model's output names\n",
    "    dynamic_axes={  # variable length axes\n",
    "        \"image\": {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"},\n",
    "        \"image_embedding\": {0: \"batch\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools.mo import convert_model\n",
    "import openvino.runtime as ov\n",
    "\n",
    "ov_encoder = convert_model(\"image_encoder.onnx\")\n",
    "ov.serialize(ov_encoder, \"openvino_fp32/image_encoder.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_dtype = model.transformer.get_cast_dtype()\n",
    "\n",
    "x = model.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "x = x + model.positional_embedding.to(cast_dtype)\n",
    "x = x.permute(1, 0, 2)  # NLD -> LND\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model.transformer,\n",
    "    (x, model.attn_mask),\n",
    "    \"text_encoder.onnx\",  # where to save the model\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],  # the model's input names\n",
    "    output_names=[\"text_embeds\"],  # the model's output names\n",
    "    dynamic_axes={  # variable length axes\n",
    "        \"input_ids\": {0: \"batch\", 1: \"sequence\"},\n",
    "        \"attention_mask\": {0: \"batch\", 1: \"sequence\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to send event with error cannot schedule new futures after shutdown.\n",
      "WARNING:root:Failed to send event with error cannot schedule new futures after shutdown.\n"
     ]
    }
   ],
   "source": [
    "from openvino.tools.mo import convert_model\n",
    "import openvino.runtime as ov\n",
    "\n",
    "ov_transformer = convert_model(\"text_encoder.onnx\")\n",
    "ov.serialize(ov_transformer, \"openvino_fp32/text_encoder.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2023.0.0-9771-d7f47aa1228\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2023.0.0-9771-d7f47aa1228\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to LATENCY.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 617.92 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     image (node: image) : f32 / [...] / [?,?,?,?]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     image_embedding (node: image_embedding) : f32 / [...] / [?,640]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'image': [1,3,240,240]\n",
      "[ INFO ] Reshape model took 66.17 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     image (node: image) : u8 / [N,C,H,W] / [1,3,240,240]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     image_embedding (node: image_embedding) : f32 / [...] / [1,640]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 1776.60 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1\n",
      "[ INFO ]   NUM_STREAMS: 1\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 18\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.LATENCY\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[ INFO ]   EXECUTION_DEVICES: ['CPU']\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'image'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'image' with random values \n",
      "[Step 10/11] Measuring performance (Start inference synchronously, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 64.59 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Execution Devices:['CPU']\n",
      "[ INFO ] Count:            1222 iterations\n",
      "[ INFO ] Duration:         60044.47 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        47.24 ms\n",
      "[ INFO ]    Average:       49.03 ms\n",
      "[ INFO ]    Min:           41.59 ms\n",
      "[ INFO ]    Max:           86.78 ms\n",
      "[ INFO ] Throughput:   21.17 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m image_encoder.onnx -shape \"image[1,3,240,240]\" -api sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2023.0.0-9771-d7f47aa1228\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2023.0.0-9771-d7f47aa1228\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to LATENCY.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 479.99 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : f32 / [...] / [?,?,640]\n",
      "[ INFO ]     attention_mask (node: attention_mask) : f32 / [...] / [?,?]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     text_embeds (node: text_embeds) : f32 / [...] / [?,?,640]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'input_ids': [77,1,640], 'attention_mask': [77,77]\n",
      "[ INFO ] Reshape model took 83.06 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : f32 / [...] / [77,1,640]\n",
      "[ INFO ]     attention_mask (node: attention_mask) : f32 / [...] / [77,77]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     text_embeds (node: text_embeds) : f32 / [...] / [77,1,640]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 868.23 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1\n",
      "[ INFO ]   NUM_STREAMS: 1\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 18\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.LATENCY\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[ INFO ]   EXECUTION_DEVICES: ['CPU']\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input_ids'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input 'attention_mask'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input_ids' with random values \n",
      "[ INFO ] Fill input 'attention_mask' with random values \n",
      "[Step 10/11] Measuring performance (Start inference synchronously, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 20.09 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Execution Devices:['CPU']\n",
      "[ INFO ] Count:            2772 iterations\n",
      "[ INFO ] Duration:         60001.04 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        21.39 ms\n",
      "[ INFO ]    Average:       21.51 ms\n",
      "[ INFO ]    Min:           17.71 ms\n",
      "[ INFO ]    Max:           33.02 ms\n",
      "[ INFO ] Throughput:   46.75 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m text_encoder.onnx -shape \"input_ids[77,1,640],attention_mask[77,77]\" -api sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/torch/onnx/symbolic_opset9.py:5408: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n",
      "/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    (image, text),\n",
    "    \"model.onnx\",  # where to save the model\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=[\"image\", \"text\"],  # the model's input names\n",
    "    output_names=[\"image_embedding\"],  # the model's output names\n",
    "    dynamic_axes={  # variable length axes\n",
    "        \"image\": {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"},\n",
    "        \"text\": {0: \"batch\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to send event with error cannot schedule new futures after shutdown.\n",
      "WARNING:root:Failed to send event with error cannot schedule new futures after shutdown.\n"
     ]
    }
   ],
   "source": [
    "from openvino.tools.mo import convert_model\n",
    "import openvino.runtime as ov\n",
    "\n",
    "ov_transformer = convert_model(\"model.onnx\")\n",
    "ov.serialize(ov_transformer, \"model.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2023.0.0-9771-d7f47aa1228\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2023.0.0-9771-d7f47aa1228\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to LATENCY.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 549.20 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     image (node: image) : f32 / [...] / [?,?,?,?]\n",
      "[ INFO ]     text (node: text) : i64 / [...] / [?,77]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     image_embedding (node: image_embedding) : f32 / [...] / [?,640]\n",
      "[ INFO ]     3690 (node: 3690) : f32 / [...] / [?,640]\n",
      "[ INFO ]     3691 (node: 3691) : f32 / [...] / []\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'image': [1,3,240,240], 'text': [1,77]\n",
      "[ INFO ] Reshape model took 113.36 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     image (node: image) : u8 / [N,C,H,W] / [1,3,240,240]\n",
      "[ INFO ]     text (node: text) : i64 / [...] / [1,77]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     image_embedding (node: image_embedding) : f32 / [...] / [1,640]\n",
      "[ INFO ]     3690 (node: 3690) : f32 / [...] / [1,640]\n",
      "[ INFO ]     3691 (node: 3691) : f32 / [...] / []\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 426.39 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1\n",
      "[ INFO ]   NUM_STREAMS: 1\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 18\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.LATENCY\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[ INFO ]   EXECUTION_DEVICES: ['CPU']\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'image'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input 'text'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'image' with random values \n",
      "[ INFO ] Fill input 'text' with random values \n",
      "[Step 10/11] Measuring performance (Start inference synchronously, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 73.00 ms\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m model.xml -shape \"image[1,3,240,240],text[1,77]\" -api sync"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomeov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
