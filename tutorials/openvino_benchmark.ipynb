{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"ViT-B-16-plus-240\"\n",
    "pretrained = \"laion400m_e32\"\n",
    "model, train_transform, eval_transform = open_clip.create_model_and_transforms(name, pretrained=pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ouptut_dir = \"openvino_fp32\"\n",
    "if not os.path.exists(ouptut_dir):\n",
    "    os.makedirs(ouptut_dir)\n",
    "    \n",
    "with open(Path(ouptut_dir) / \"model_index.txt\", 'w') as fd:\n",
    "    fd.write(f\"{name}, {pretrained}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = open_clip.get_tokenizer('ViT-B-16-plus-240')\n",
    "\n",
    "image = eval_transform(Image.open(\"../docs/CLIP.png\")).unsqueeze(0)\n",
    "text = tokenizer(\"a cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model.visual,\n",
    "    image,\n",
    "    \"image_encoder.onnx\",  # where to save the model\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=[\"image\"],  # the model's input names\n",
    "    output_names=[\"image_embedding\"],  # the model's output names\n",
    "    dynamic_axes={  # variable length axes\n",
    "        \"image\": {0: \"batch\"},\n",
    "        \"image_embedding\": {0: \"batch\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools.mo import convert_model\n",
    "import openvino.runtime as ov\n",
    "\n",
    "ov_encoder = convert_model(\"image_encoder.onnx\")\n",
    "ov.serialize(ov_encoder, ouptut_dir + \"/image_encoder.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_dtype = model.transformer.get_cast_dtype()\n",
    "\n",
    "x = model.token_embedding(text).to(cast_dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "x = x + model.positional_embedding.to(cast_dtype)\n",
    "x = x.permute(1, 0, 2)  # NLD -> LND\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model.transformer,\n",
    "    (x, model.attn_mask),\n",
    "    \"text_encoder.onnx\",  # where to save the model\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],  # the model's input names\n",
    "    output_names=[\"text_embeds\"],  # the model's output names\n",
    "    dynamic_axes={  # variable length axes\n",
    "        \"input_ids\": {1: \"batch\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools.mo import convert_model\n",
    "import openvino.runtime as ov\n",
    "\n",
    "ov_transformer = convert_model(\"text_encoder.onnx\")\n",
    "ov.serialize(ov_transformer, ouptut_dir + \"/text_encoder.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to LATENCY.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 598.39 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     image (node: image) : f32 / [...] / [?,3,240,240]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     image_embedding (node: image_embedding) : f32 / [...] / [?,640]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'image': [1,3,240,240]\n",
      "[ INFO ] Reshape model took 42.74 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     image (node: image) : u8 / [N,C,H,W] / [1,3,240,240]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     image_embedding (node: image_embedding) : f32 / [...] / [1,640]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 1173.27 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1\n",
      "[ INFO ]   NUM_STREAMS: 1\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 18\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.LATENCY\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'image'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'image' with random values \n",
      "[Step 10/11] Measuring performance (Start inference synchronously, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 49.43 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            1393 iterations\n",
      "[ INFO ] Duration:         60005.38 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        43.01 ms\n",
      "[ INFO ]    Average:       42.97 ms\n",
      "[ INFO ]    Min:           40.63 ms\n",
      "[ INFO ]    Max:           46.48 ms\n",
      "[ INFO ] Throughput:   23.25 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m image_encoder.onnx -shape \"image[1,3,240,240]\" -api sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to LATENCY.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 354.08 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : f32 / [...] / [77,?,640]\n",
      "[ INFO ]     attention_mask (node: attention_mask) : f32 / [...] / [77,77]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     text_embeds (node: text_embeds) : f32 / [...] / [77,?,640]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'input_ids': [77,1,640], 'attention_mask': [77,77]\n",
      "[ INFO ] Reshape model took 43.51 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : f32 / [...] / [77,1,640]\n",
      "[ INFO ]     attention_mask (node: attention_mask) : f32 / [...] / [77,77]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     text_embeds (node: text_embeds) : f32 / [...] / [77,1,640]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 659.61 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1\n",
      "[ INFO ]   NUM_STREAMS: 1\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 18\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.LATENCY\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input_ids'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input 'attention_mask'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input_ids' with random values \n",
      "[ INFO ] Fill input 'attention_mask' with random values \n",
      "[Step 10/11] Measuring performance (Start inference synchronously, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 22.69 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            2604 iterations\n",
      "[ INFO ] Duration:         60005.13 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        23.14 ms\n",
      "[ INFO ]    Average:       22.91 ms\n",
      "[ INFO ]    Min:           18.28 ms\n",
      "[ INFO ]    Max:           24.35 ms\n",
      "[ INFO ] Throughput:   43.22 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m text_encoder.onnx -shape \"input_ids[77,1,640],attention_mask[77,77]\" -api sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    (image, text),\n",
    "    \"model.onnx\",  # where to save the model\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=[\"image\", \"text\"],  # the model's input names\n",
    "    output_names=[\"image_embedding\"],  # the model's output names\n",
    "    dynamic_axes={  # variable length axes\n",
    "        \"image\": {0: \"batch\"},\n",
    "        \"text\": {0: \"batch\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools.mo import convert_model\n",
    "import openvino.runtime as ov\n",
    "\n",
    "ov_transformer = convert_model(\"model.onnx\")\n",
    "ov.serialize(ov_transformer, \"model.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to LATENCY.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 523.88 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     image (node: image) : f32 / [...] / [?,3,240,240]\n",
      "[ INFO ]     text (node: text) : i64 / [...] / [?,77]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     image_embedding (node: image_embedding) : f32 / [...] / [?,640]\n",
      "[ INFO ]     3690 (node: 3690) : f32 / [...] / [?,640]\n",
      "[ INFO ]     3691 (node: 3691) : f32 / [...] / []\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'image': [1,3,240,240], 'text': [1,77]\n",
      "[ INFO ] Reshape model took 78.64 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     image (node: image) : u8 / [N,C,H,W] / [1,3,240,240]\n",
      "[ INFO ]     text (node: text) : i64 / [...] / [1,77]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     image_embedding (node: image_embedding) : f32 / [...] / [1,640]\n",
      "[ INFO ]     3690 (node: 3690) : f32 / [...] / [1,640]\n",
      "[ INFO ]     3691 (node: 3691) : f32 / [...] / []\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 437.16 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 1\n",
      "[ INFO ]   NUM_STREAMS: 1\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 18\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.LATENCY\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'image'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input 'text'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'image' with random values \n",
      "[ INFO ] Fill input 'text' with random values \n",
      "[Step 10/11] Measuring performance (Start inference synchronously, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 65.82 ms\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alex/virt_envs/tomeov/bin/benchmark_app\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/openvino/tools/benchmark/main.py\", line 563, in main\n",
      "    fps, median_latency_ms, avg_latency_ms, min_latency_ms, max_latency_ms, total_duration_sec, iteration = benchmark.main_loop(requests, data_queue, batch_size, args.latency_percentile, pcseq)\n",
      "  File \"/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/openvino/tools/benchmark/benchmark.py\", line 167, in main_loop\n",
      "    times, total_duration_sec, iteration = self.sync_inference(requests[0], data_queue)\n",
      "  File \"/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/openvino/tools/benchmark/benchmark.py\", line 98, in sync_inference\n",
      "    request.infer()\n",
      "  File \"/home/alex/virt_envs/tomeov/lib/python3.8/site-packages/openvino/runtime/ie_api.py\", line 145, in infer\n",
      "    return super().infer({})\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m model.xml -shape \"image[1,3,240,240],text[1,77]\" -api sync"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomeov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
